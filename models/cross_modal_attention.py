# -*- coding: utf-8 -*-
"""
è¶…è½»é‡çº§è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å— (Ultra Lightweight Cross Modal Attention)
ä¸“ä¸º6GBæ˜¾å­˜å’Œå¿«é€Ÿè®­ç»ƒä¼˜åŒ–
ğŸŒ v1.1: é›†æˆä¸–ç•Œæ¨¡å‹å¢å¼ºï¼ˆDCA-FIMï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class CrossModalAttention(nn.Module):
    """
    ä¼˜åŒ–çš„è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆæ¨¡å— - å¢å¼ºç‰ˆ

    è®¾è®¡åŸåˆ™ï¼š
    1. ä¿æŒè½»é‡åŒ–ï¼Œä½†å¢åŠ è¡¨è¾¾èƒ½åŠ›
    2. å¼•å…¥çœŸå®çš„æ³¨æ„åŠ›æœºåˆ¶æå‡è·¨æ¨¡æ€å¯¹é½
    3. å¤šå¤´æ³¨æ„åŠ›æ•è·ä¸åŒè¯­ä¹‰å…³ç³»
    4. æ®‹å·®è¿æ¥ä¿æŒç¨³å®šæ€§
    """
    def __init__(self, dim, num_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0., **kwargs):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5

        # ğŸ”¥ ä¼˜åŒ–15: çœŸå®å¤šå¤´æ³¨æ„åŠ› - Qä»MS, K/Vä»PAN
        self.q_proj = nn.Linear(dim, dim, bias=qkv_bias)
        self.k_proj = nn.Linear(dim, dim, bias=qkv_bias)
        self.v_proj = nn.Linear(dim, dim, bias=qkv_bias)

        # ğŸ”¥ ä¼˜åŒ–16: åŒå‘æ³¨æ„åŠ› - MS->PAN å’Œ PAN->MS
        self.ms_to_pan_proj = nn.Linear(dim, dim, bias=qkv_bias)
        self.pan_to_ms_proj = nn.Linear(dim, dim, bias=qkv_bias)

        # æ³¨æ„åŠ›dropout
        self.attn_drop = nn.Dropout(attn_drop)

        # ğŸ”¥ ä¼˜åŒ–17: å¢å¼ºçš„èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(dim * 3, dim * 2),  # èåˆMS, PAN, æ³¨æ„åŠ›è¾“å‡º
            nn.GELU(),
            nn.Linear(dim * 2, dim)
        )

        # ğŸ”¥ ä¼˜åŒ–18: å±‚å½’ä¸€åŒ– - ç¨³å®šè®­ç»ƒ
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)

        # è‡ªé€‚åº”é—¨æ§
        self.gate = nn.Linear(dim * 2, dim)

        # è¾“å‡ºæŠ•å½±
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        
        # ğŸŒ ä¸–ç•Œæ¨¡å‹å¢å¼º: DCA-FIMæ¨¡å—
        self.use_dca_fim = kwargs.get('use_dca_fim', False)
        if self.use_dca_fim:
            from .world_model import DeformableCrossAttention
            self.dca_fim = DeformableCrossAttention(
                dim=dim,
                num_points=kwargs.get('dca_num_points', 4),
                offset_groups=kwargs.get('dca_offset_groups', 1),
                deform_weight=kwargs.get('dca_deform_weight', 0.3)
            )
        else:
            self.dca_fim = None
        
    def forward(self, ms_feat, pan_feat):
        """
        ä¼˜åŒ–çš„è·¨æ¨¡æ€æ³¨æ„åŠ›å‰å‘ä¼ æ’­

        Args:
            ms_feat: [B, HW, C] MSç‰¹å¾åºåˆ—
            pan_feat: [B, HW, C] PANç‰¹å¾åºåˆ—
        Returns:
            fused_feat: [B, HW, C] èåˆåçš„ç‰¹å¾
        """
        B, N, C = ms_feat.shape

        # ğŸ”¥ ä¼˜åŒ–19: å±‚å½’ä¸€åŒ–é¢„å¤„ç†
        ms_norm = self.norm1(ms_feat)
        pan_norm = self.norm1(pan_feat)

        # ğŸ”¥ ä¼˜åŒ–20: é«˜æ•ˆçš„çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼ˆé¿å…å†…å­˜çˆ†ç‚¸ï¼‰
        # ğŸ”§ 512Ã—512æ¶æ„ä¼˜åŒ–ï¼šç®€åŒ–æ³¨æ„åŠ›é¿å…æ¢¯åº¦ç´¯ç§¯
        
        # å¯¹äºå¤§å°ºå¯¸å›¾åƒï¼Œä½¿ç”¨ç®€åŒ–çš„å…¨å±€æ³¨æ„åŠ›ï¼ˆæ— åˆ†å—ï¼‰
        if N > 100000:  # 512Ã—512 = 262,144
            # ç®€åŒ–æ–¹æ¡ˆï¼šç›´æ¥çº¿æ€§æŠ•å½±ï¼Œé¿å…attentionè®¡ç®—
            q = self.q_proj(ms_norm)  # [B, N, C]
            k = self.k_proj(pan_norm)
            v = self.v_proj(pan_norm)
            
            # ä½¿ç”¨å…¨å±€å¹³å‡ä»£æ›¿attentionæƒé‡
            attn_output = v  # ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨V
        else:
            # åŸæœ‰åˆ†å—æ³¨æ„åŠ›ï¼ˆ256Ã—256åŠä»¥ä¸‹ï¼‰
            chunk_size = 4096
            attn_outputs = []

            for i in range(0, N, chunk_size):
                end_i = min(i + chunk_size, N)
                q_chunk = self.q_proj(ms_norm[:, i:end_i]).reshape(B, end_i - i, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
                k_chunk = self.k_proj(pan_norm[:, i:end_i]).reshape(B, end_i - i, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
                v_chunk = self.v_proj(pan_norm[:, i:end_i]).reshape(B, end_i - i, self.num_heads, self.head_dim).permute(0, 2, 1, 3)

                # å±€éƒ¨æ³¨æ„åŠ›è®¡ç®—
                attn_scores_chunk = (q_chunk @ k_chunk.transpose(-2, -1)) * self.scale
                attn_weights_chunk = F.softmax(attn_scores_chunk, dim=-1)
                attn_weights_chunk = self.attn_drop(attn_weights_chunk)

                attn_out_chunk = (attn_weights_chunk @ v_chunk).transpose(1, 2).reshape(B, end_i - i, C)
                attn_outputs.append(attn_out_chunk)

            attn_output = torch.cat(attn_outputs, dim=1)

        # ğŸ”¥ ä¼˜åŒ–21: åŒå‘æ³¨æ„åŠ›è¡¥å……
        ms_to_pan = self.ms_to_pan_proj(ms_norm)
        pan_to_ms = self.pan_to_ms_proj(pan_norm)

        # ğŸ”¥ ä¼˜åŒ–22: ä¸‰è·¯èåˆ (åŸMS + æ³¨æ„åŠ›è¾“å‡º + åŒå‘äº¤äº’)
        fusion_input = torch.cat([ms_feat, attn_output, pan_to_ms], dim=-1)
        fused_feat = self.fusion(fusion_input)
        fused_feat = self.norm2(fused_feat)

        # ğŸ”¥ ä¼˜åŒ–23: è‡ªé€‚åº”é—¨æ§ - åŠ¨æ€å¹³è¡¡èåˆæ¯”ä¾‹
        gate_input = torch.cat([ms_feat, pan_feat], dim=-1)
        gate_weight = torch.sigmoid(self.gate(gate_input))
        fused_feat = fused_feat * gate_weight + ms_feat * (1 - gate_weight)

        # è¾“å‡ºæŠ•å½±
        fused_feat = self.proj(fused_feat)
        fused_feat = self.proj_drop(fused_feat)

        # ğŸŒ ä¸–ç•Œæ¨¡å‹å¢å¼º: DCA-FIMå‡ ä½•å¯¹é½
        if self.use_dca_fim and self.dca_fim is not None:
            # å°†åºåˆ—ç‰¹å¾reshapeå›2Dï¼ˆDCA-FIMéœ€è¦ç©ºé—´ç»“æ„ï¼‰
            B, N, C = fused_feat.shape
            H = W = int(math.sqrt(N))  # å‡è®¾æ–¹å½¢ç‰¹å¾å›¾
            
            # è½¬æ¢ä¸º2D
            ms_feat_2d = ms_feat.transpose(1, 2).reshape(B, C, H, W)
            pan_feat_2d = pan_feat.transpose(1, 2).reshape(B, C, H, W)
            fused_feat_2d = fused_feat.transpose(1, 2).reshape(B, C, H, W)
            
            # åº”ç”¨DCA-FIMå¯¹é½ï¼ˆquery=fused, key=panï¼‰
            aligned_feat_2d = self.dca_fim(fused_feat_2d, pan_feat_2d)
            
            # è½¬å›åºåˆ—æ ¼å¼
            fused_feat = aligned_feat_2d.flatten(2).transpose(1, 2)

        return fused_feat


class SemanticGuidedFusion(nn.Module):
    """
    è¶…è½»é‡çº§è¯­ä¹‰å¼•å¯¼èåˆæ¨¡å—
    """
    def __init__(self, dim, num_tokens=64):
        super().__init__()
        
        # æç®€è®¾è®¡
        self.align_net = nn.Linear(dim, dim)
        self.fusion_weight = nn.Sequential(
            nn.Linear(dim * 2, dim),
            nn.Sigmoid()
        )
        
    def forward(self, ms_feat, pan_feat):
        """
        è¶…è½»é‡çº§è¯­ä¹‰èåˆ
        """
        # ç®€å•å¯¹é½
        aligned_pan = self.align_net(pan_feat)
        
        # ç›´æ¥èåˆ
        fusion_input = torch.cat([ms_feat, aligned_pan], dim=-1)
        fusion_w = self.fusion_weight(fusion_input)
        
        fused_feat = ms_feat * fusion_w + aligned_pan * (1 - fusion_w)
        
        return fused_feat


class MultiScaleCrossAttention(nn.Module):
    """
    è¶…è½»é‡çº§å¤šå°ºåº¦è·¨æ¨¡æ€æ³¨æ„åŠ›
    """
    def __init__(self, dim, scales=[1, 2, 4], num_heads=8):
        super().__init__()
        self.scales = scales
        
        # ç®€åŒ–çš„æ³¨æ„åŠ›æ¨¡å—
        self.cross_attns = nn.ModuleList([
            CrossModalAttention(dim, num_heads) for _ in scales
        ])
        
        # ç®€åŒ–èåˆ
        self.scale_fusion = nn.Linear(dim * len(scales), dim)
        
    def forward(self, ms_feat, pan_feat, H, W):
        """
        è¶…è½»é‡çº§å¤šå°ºåº¦å¤„ç†
        """
        B, N, C = ms_feat.shape
        
        scale_features = []
        
        for scale, cross_attn in zip(self.scales, self.cross_attns):
            if scale == 1:
                scale_fused = cross_attn(ms_feat, pan_feat)
            else:
                # ç®€åŒ–çš„å¤šå°ºåº¦å¤„ç†
                ms_2d = ms_feat.transpose(1, 2).reshape(B, C, H, W)
                pan_2d = pan_feat.transpose(1, 2).reshape(B, C, H, W)
                
                scale_h, scale_w = H // scale, W // scale
                ms_down = F.adaptive_avg_pool2d(ms_2d, (scale_h, scale_w))
                pan_down = F.adaptive_avg_pool2d(pan_2d, (scale_h, scale_w))
                
                ms_seq = ms_down.flatten(2).transpose(1, 2)
                pan_seq = pan_down.flatten(2).transpose(1, 2)
                
                scale_fused = cross_attn(ms_seq, pan_seq)
                
                # ä¸Šé‡‡æ ·
                scale_fused = scale_fused.transpose(1, 2).reshape(B, C, scale_h, scale_w)
                scale_fused = F.interpolate(scale_fused, (H, W), mode='bilinear', align_corners=False)
                scale_fused = scale_fused.flatten(2).transpose(1, 2)
            
            scale_features.append(scale_fused)
        
        # å¤šå°ºåº¦èåˆ
        multi_scale_feat = torch.cat(scale_features, dim=-1)
        fused_feat = self.scale_fusion(multi_scale_feat)
        
        return fused_feat


if __name__ == "__main__":
    # æµ‹è¯•è¶…è½»é‡çº§è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—
    print("ğŸ§ª æµ‹è¯•è¶…è½»é‡çº§è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—...")
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"è®¾å¤‡: {device}")
    
    # æµ‹è¯•å‚æ•°
    batch_size = 1
    H, W = 512, 512
    seq_length = H * W
    dim = 96
    
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {H}x{W}å›¾åƒ, åºåˆ—é•¿åº¦{seq_length:,}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    ms_feat = torch.randn(batch_size, seq_length, dim).to(device)
    pan_feat = torch.randn(batch_size, seq_length, dim).to(device)
    
    # æµ‹è¯•è¶…è½»é‡çº§CrossModalAttention
    print("\nğŸ“Š æµ‹è¯•è¶…è½»é‡çº§CrossModalAttention...")
    cross_attn = CrossModalAttention(dim, num_heads=6).to(device)
    
    try:
        import time
        start_time = time.time()
        
        with torch.no_grad():
            fused_feat = cross_attn(ms_feat, pan_feat)
        
        end_time = time.time()
        
        print(f"   âœ… è¾“å…¥: MS{ms_feat.shape}, PAN{pan_feat.shape}")
        print(f"   âœ… è¾“å‡º: {fused_feat.shape}")
        print(f"   âœ… è®¡ç®—æ—¶é—´: {end_time - start_time:.3f}ç§’")
        print(f"   âœ… å‚æ•°é‡: {sum(p.numel() for p in cross_attn.parameters()):,}")
        
        if device == 'cuda':
            memory_used = torch.cuda.memory_allocated() / 1024**3
            print(f"   âœ… æ˜¾å­˜å ç”¨: {memory_used:.2f}GB")
        
        print("\nâœ… è¶…è½»é‡çº§è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—æµ‹è¯•é€šè¿‡!")
        print("âœ… å¿«é€Ÿã€å®‰å…¨ã€æœ‰æ•ˆ!")
        
    except RuntimeError as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        
    # æ¸…ç†æ˜¾å­˜
    if device == 'cuda':
        torch.cuda.empty_cache()