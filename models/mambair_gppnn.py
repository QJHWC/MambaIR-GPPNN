# -*- coding: utf-8 -*-
"""
MambaIR-GPPNN ç½‘ç»œæ¶æ„
åŸºäº MambaIRv2 (Base/Large) çš„å¤šæ¨¡æ€èåˆæ¨¡å‹ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

from .dual_modal_assm import DualModal_ASSM
from .cross_modal_attention import CrossModalAttention


class MambaIRv2_GPPNN(nn.Module):
    """
    MambaIRv2 ä¸ GPPNN çš„èåˆæ¶æ„ï¼Œä¿æŒåŸæœ‰ GPPNN æ¥å£ã€‚
    """
    def __init__(self,
                 ms_channels=3,
                 pan_channels=1,
                 embed_dim=96,
                 d_state=16,
                 depths=[6, 6, 6, 6],
                 num_heads=[6, 6, 6, 6],
                 window_size=8,
                 inner_rank=32,
                 num_tokens=128,
                 mlp_ratio=2.0,
                 model_size='base',
                 **kwargs):  # ğŸŒ æ¥å—ä¸–ç•Œæ¨¡å‹é…ç½®å‚æ•°
        super().__init__()

        if model_size == 'large':
            embed_dim = 128
            d_state = 20
            depths = [8, 8, 8, 8]
            num_heads = [8, 8, 8, 8]
            inner_rank = 48
            num_tokens = 256

        self.embed_dim = embed_dim
        self.model_size = model_size

        print(f"[Init] MambaIRv2-GPPNN ({model_size.upper()})")
        print(f"  embed_dim={embed_dim}, d_state={d_state}")
        print(f"  depths={depths}, num_heads={num_heads}")

        # 1. æµ…å±‚ç‰¹å¾æå–
        self.ms_conv_first = nn.Conv2d(ms_channels, embed_dim, 3, 1, 1)
        self.pan_conv_first = nn.Conv2d(pan_channels, embed_dim, 3, 1, 1)

        # 2. MambaIRv2 æ¨¡å—
        self.mamba_layers = nn.ModuleList()
        current_dim = embed_dim

        for i, (depth, num_head) in enumerate(zip(depths, num_heads)):
            layer_d_state = d_state + i * 2
            layer_tokens = num_tokens + i * 64
            layer_rank = inner_rank + i * 8

            layer = DualModal_ASSM(
                dim=current_dim,
                d_state=layer_d_state,
                num_tokens=layer_tokens,
                inner_rank=layer_rank,
                mlp_ratio=mlp_ratio
            )
            self.mamba_layers.append(layer)

        # 3. è·¨æ¨¡æ€æ³¨æ„åŠ› - ğŸ”¥ ä¼˜åŒ–24: é€å±‚å¢åŠ æ³¨æ„åŠ›å¤´æ•°
        # ğŸŒ ä¼ é€’ä¸–ç•Œæ¨¡å‹é…ç½®ï¼ˆDCA-FIMï¼‰
        cross_attn_kwargs = {
            'use_dca_fim': kwargs.get('use_dca_fim', False),
            'dca_num_points': kwargs.get('dca_num_points', 4),
            'dca_offset_groups': kwargs.get('dca_offset_groups', 1),
            'dca_deform_weight': kwargs.get('dca_deform_weight', 0.3),
        }
        
        self.cross_modal_layers = nn.ModuleList([
            CrossModalAttention(embed_dim, num_heads=6, attn_drop=0.1, proj_drop=0.1, **cross_attn_kwargs),
            CrossModalAttention(embed_dim, num_heads=8, attn_drop=0.1, proj_drop=0.1, **cross_attn_kwargs),
            CrossModalAttention(embed_dim, num_heads=8, attn_drop=0.1, proj_drop=0.1, **cross_attn_kwargs)
        ])
        # ğŸ”¥ ä¼˜åŒ–25: å¢å¼ºä½å±‚ç»†èŠ‚ä¿ç•™
        self.low_level_refine = nn.Sequential(
            nn.Conv2d(embed_dim, embed_dim, 3, 1, 1, groups=embed_dim),
            nn.GELU(),
            nn.Conv2d(embed_dim, embed_dim, 1, 1, 0)
        )
        # ğŸ”¥ ä¼˜åŒ–26: è¾¹ç¼˜ä¿æŠ¤æ¨¡å—
        self.edge_preserve = nn.Sequential(
            nn.Conv2d(embed_dim, embed_dim // 4, 1, 1, 0),
            nn.GELU(),
            nn.Conv2d(embed_dim // 4, embed_dim, 1, 1, 0),
            nn.Sigmoid()
        )

        # 4. å¤šå°ºåº¦å¤„ç†
        self.downsample = nn.AvgPool2d(2, 2)
        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

        # 5. ç‰¹å¾èåˆ - ğŸ”¥ ä¼˜åŒ–27: ä½¿ç”¨GELU + æ®‹å·®å—
        self.fusion_conv1 = nn.Sequential(
            nn.Conv2d(embed_dim * 2, embed_dim * 2, 3, 1, 1),
            nn.GELU(),
            nn.Conv2d(embed_dim * 2, embed_dim * 2, 3, 1, 1),
            nn.GELU(),
            nn.Conv2d(embed_dim * 2, embed_dim * 2, 1, 1, 0)
        )
        self.fusion_conv2 = nn.Sequential(
            nn.Conv2d(embed_dim * 2, embed_dim * 4, 3, 1, 1),
            nn.GELU(),
            nn.Conv2d(embed_dim * 4, embed_dim * 4, 3, 1, 1),
            nn.GELU(),
            nn.Conv2d(embed_dim * 4, embed_dim * 4, 1, 1, 0)
        )
        # ğŸ”¥ ä¼˜åŒ–28: åŒè·¯å…¨å±€ä¸Šä¸‹æ–‡ï¼ˆé¢‘é“+ç©ºé—´ï¼‰
        self.global_context = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(embed_dim * 4, embed_dim, 1, 1, 0),
            nn.GELU(),
            nn.Conv2d(embed_dim, embed_dim * 4, 1, 1, 0),
            nn.Sigmoid()
        )
        # ğŸ”¥ ä¼˜åŒ–29: ç©ºé—´æ³¨æ„åŠ›
        self.spatial_attn = nn.Sequential(
            nn.Conv2d(embed_dim * 4, 1, 7, 1, 3),
            nn.Sigmoid()
        )

        # 6. è§£ç 
        self.deconv1 = nn.ConvTranspose2d(embed_dim * 4, embed_dim * 2, 2, 2, 0)
        self.deconv2 = nn.ConvTranspose2d(embed_dim * 3, embed_dim, 2, 2, 0)

        # 7. æ®‹å·®å¼ºåŒ–
        self.residual_enhance = nn.Sequential(
            nn.Conv2d(embed_dim * 2, embed_dim * 2, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(embed_dim * 2, embed_dim * 2, 1, 1, 0)
        )

        # 8. å¤šå°ºåº¦è¾“å‡º
        self.output_conv1 = nn.Sequential(
            nn.Conv2d(embed_dim * 4, ms_channels, 3, 1, 1),
            nn.Sigmoid()
        )
        self.output_conv2 = nn.Sequential(
            nn.Conv2d(embed_dim * 2, ms_channels, 3, 1, 1),
            nn.Sigmoid()
        )
        self.output_conv3 = nn.Sequential(
            nn.Conv2d(embed_dim, ms_channels, 3, 1, 1),
            nn.Sigmoid()
        )

        # ğŸŒ ä¸–ç•Œæ¨¡å‹å¢å¼º: WSMæ¨¡å—
        self.use_wsm = kwargs.get('use_wsm', False)
        if self.use_wsm:
            from .world_model import WorldStateMemory
            self.wsm = WorldStateMemory(
                feature_dim=embed_dim,
                hidden_dim=kwargs.get('wsm_hidden_dim', 128),
                dropout=kwargs.get('wsm_dropout', 0.1),
                layer_scale_init=kwargs.get('wsm_layer_scale_init', 0.1)
            )
            print(f"[Init] [WorldModel] WSM (World State Memory) enabled, hidden_dim={kwargs.get('wsm_hidden_dim', 128)}")
        else:
            self.wsm = None

        self._init_weights()

    def _init_weights(self):
        """MambaIRv2 é£æ ¼çš„æƒé‡åˆå§‹åŒ–ã€‚"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.ConvTranspose2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

    def forward(self, ms, pan):
        """
        å‰å‘ä¼ æ’­ï¼Œè¾“å‡ºä¿æŒä¸åŸ GPPNN ä¸€è‡´çš„ä¸‰ä¸ªå°ºåº¦ã€‚
        """
        if pan is None:
            raise Exception('PAN image is required for pansharpening!')

        B, _, H, W = ms.shape

        # æµ…å±‚ç‰¹å¾
        ms_feat1 = F.relu(self.ms_conv_first(ms))
        pan_feat1 = F.relu(self.pan_conv_first(pan))

        # Stage 1: å…¨åˆ†è¾¨ç‡ - ğŸ”¥ ä¼˜åŒ–30: å¢å¼ºç»†èŠ‚ä¿æŠ¤
        ms_enhanced1, pan_enhanced1 = self.mamba_layers[0](ms_feat1, pan_feat1, (H, W))
        ms_seq1 = ms_enhanced1.flatten(2).transpose(1, 2)
        pan_seq1 = pan_enhanced1.flatten(2).transpose(1, 2)
        fused_seq1 = self.cross_modal_layers[0](ms_seq1, pan_seq1)
        fused_feat1 = fused_seq1.transpose(1, 2).reshape(B, self.embed_dim, H, W)

        # ğŸŒ ä¸–ç•Œæ¨¡å‹å¢å¼º: WSMçŠ¶æ€è°ƒåˆ¶
        if self.use_wsm and self.wsm is not None:
            # è·å–å‰ä¸€æ—¶åˆ»éšçŠ¶æ€ï¼ˆé¦–æ¬¡ä¸ºNoneï¼‰
            h_prev = getattr(self, '_wsm_hidden_state', None)
            # WSMè°ƒåˆ¶: F' = F * (1 + gamma) + beta
            fused_feat1, h_t, gamma, beta = self.wsm(fused_feat1, h_prev)
            # ä¿å­˜éšçŠ¶æ€ä¾›ä¸‹ä¸€æ¬¡ä½¿ç”¨
            if self.training:
                # è®­ç»ƒæ—¶detaché¿å…é•¿æœŸæ¢¯åº¦ç´¯ç§¯
                self._wsm_hidden_state = h_t.detach()
            else:
                # æ¨ç†æ—¶ä¿æŒå®Œæ•´çŠ¶æ€
                self._wsm_hidden_state = h_t

        # ğŸ”¥ ä¼˜åŒ–31: è¾¹ç¼˜ä¿æŠ¤ + ä½å±‚ç»†åŒ–
        refined_feat1 = self.low_level_refine(fused_feat1)
        edge_weight = self.edge_preserve(fused_feat1)
        fused_feat1 = fused_feat1 + refined_feat1 * edge_weight

        # ğŸ”¥ ä¼˜åŒ–32: è°ƒæ•´æ®‹å·®æ¯”ä¾‹ï¼Œå¢å¼ºèåˆ
        ms_enhanced1 = ms_enhanced1 + fused_feat1 * 0.35
        pan_enhanced1 = pan_enhanced1 + fused_feat1 * 0.35

        # Stage 2: 1/2 åˆ†è¾¨ç‡
        ms_feat2 = self.downsample(ms_enhanced1)
        pan_feat2 = self.downsample(pan_enhanced1)
        target_h = max(H // 2, 1)
        target_w = max(W // 2, 1)
        if len(self.mamba_layers) > 1:
            ms_enhanced2, pan_enhanced2 = self.mamba_layers[1](ms_feat2, pan_feat2, (target_h, target_w))
        else:
            ms_enhanced2, pan_enhanced2 = ms_feat2, pan_feat2

        if len(self.cross_modal_layers) > 1:
            ms_seq2 = ms_enhanced2.flatten(2).transpose(1, 2)
            pan_seq2 = pan_enhanced2.flatten(2).transpose(1, 2)
            fused_seq2 = self.cross_modal_layers[1](ms_seq2, pan_seq2)
            fused_feat2_residual = fused_seq2.transpose(1, 2).reshape(B, self.embed_dim, target_h, target_w)
            ms_enhanced2 = ms_enhanced2 + fused_feat2_residual * 0.25
            pan_enhanced2 = pan_enhanced2 + fused_feat2_residual * 0.25

        fused_feat2 = torch.cat([ms_enhanced2, pan_enhanced2], dim=1)
        fused_feat2 = self.fusion_conv1(fused_feat2)

        # Stage 3/4: 1/4 åˆ†è¾¨ç‡
        low_h = max(H // 4, 1)
        low_w = max(W // 4, 1)
        low_size = (low_h, low_w)
        fused_feat4 = F.adaptive_avg_pool2d(fused_feat2, low_size)
        low_ms = F.adaptive_avg_pool2d(ms_enhanced2, low_size)
        low_pan = F.adaptive_avg_pool2d(pan_enhanced2, low_size)

        if len(self.mamba_layers) > 2:
            low_ms, low_pan = self.mamba_layers[2](low_ms, low_pan, low_size)
        if len(self.mamba_layers) > 3:
            low_ms, low_pan = self.mamba_layers[3](low_ms, low_pan, low_size)

        if len(self.cross_modal_layers) > 2:
            low_seq_ms = low_ms.flatten(2).transpose(1, 2)
            low_seq_pan = low_pan.flatten(2).transpose(1, 2)
            low_fused_seq = self.cross_modal_layers[2](low_seq_ms, low_seq_pan)
            low_fused_feat = low_fused_seq.transpose(1, 2).reshape(B, self.embed_dim, low_h, low_w)
            low_ms = low_ms + low_fused_feat * 0.25
            low_pan = low_pan + low_fused_feat * 0.25

        low_fused = torch.cat([low_ms, low_pan], dim=1)
        fused_feat4 = fused_feat4 + low_fused
        fused_feat4 = fused_feat4 + self.residual_enhance(fused_feat4) * 0.1
        fused_feat4 = self.fusion_conv2(fused_feat4)

        # ğŸ”¥ ä¼˜åŒ–33: åŒè·¯æ³¨æ„åŠ›å¢å¼ºï¼ˆé¢‘é“ + ç©ºé—´ï¼‰
        context_weight = self.global_context(fused_feat4)
        spatial_weight = self.spatial_attn(fused_feat4)
        fused_feat4 = fused_feat4 * (1 + context_weight) * (1 + spatial_weight * 0.5)

        # è¾“å‡ºé‡‘å­—å¡”
        output_1_4 = self.output_conv1(fused_feat4)

        deconv1 = F.relu(self.deconv1(fused_feat4))
        deconv1_cat = torch.cat([deconv1, fused_feat2], dim=1)
        output_1_2 = self.output_conv2(deconv1_cat[:, :self.embed_dim * 2])

        deconv2 = F.relu(self.deconv2(deconv1_cat[:, :self.embed_dim * 3]))
        deconv2_cat = torch.cat([deconv2, fused_feat1], dim=1)
        output_full = self.output_conv3(deconv2_cat[:, :self.embed_dim])

        down_1_2 = F.avg_pool2d(output_full, 2, 2)
        down_1_4 = F.avg_pool2d(down_1_2, 2, 2)

        SR_1_4 = [output_1_4, down_1_4]
        SR_1_2 = [output_1_2, down_1_2]

        return SR_1_4, SR_1_2, output_full


def create_mambairv2_gppnn(model_size='base', **kwargs):
    """
    æ„å»ºæŒ‡å®šè§„æ¨¡çš„ MambaIRv2-GPPNN æ¨¡å‹
    
    Args:
        model_size: æ¨¡å‹å¤§å° ('base' or 'large')
        **kwargs: é¢å¤–å‚æ•°ï¼ˆåŒ…æ‹¬ä¸–ç•Œæ¨¡å‹é…ç½®ï¼‰
            - use_wsm: å¯ç”¨ä¸–ç•ŒçŠ¶æ€è®°å¿†
            - wsm_hidden_dim: WSMéšçŠ¶æ€ç»´åº¦
            - use_dca_fim: å¯ç”¨å¯å½¢å˜å¯¹é½
            - å…¶ä»–ä¸–ç•Œæ¨¡å‹å‚æ•°...
    """
    if model_size == 'base':
        return MambaIRv2_GPPNN(
            embed_dim=96,
            d_state=16,
            depths=[6, 6, 6, 6],
            num_heads=[6, 6, 6, 6],
            model_size='base',
            **kwargs
        )
    elif model_size == 'large':
        return MambaIRv2_GPPNN(
            embed_dim=128,
            d_state=20,
            depths=[8, 8, 8, 8],
            num_heads=[8, 8, 8, 8],
            model_size='large',
            **kwargs
        )
    else:
        raise ValueError(f"Unsupported model_size: {model_size}")


if __name__ == "__main__":
    print("==> æ„å»º MambaIRv2-GPPNN æ¨¡å‹...")

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    batch_size = 2
    height = width = 256
    ms = torch.randn(batch_size, 3, height, width).to(device)
    pan = torch.randn(batch_size, 1, height, width).to(device)

    print("\n[Base] æ¨¡å‹:")
    model_base = create_mambairv2_gppnn('base').to(device)
    base_params = sum(p.numel() for p in model_base.parameters())
    with torch.no_grad():
        SR_1_4, SR_1_2, output = model_base(ms, pan)
    print(f"  å‚æ•°é‡: {base_params:,}")
    print(f"  è¾“å‡ºå°ºå¯¸: {output.shape}")

    print("\n[Large] æ¨¡å‹:")
    model_large = create_mambairv2_gppnn('large').to(device)
    large_params = sum(p.numel() for p in model_large.parameters())
    with torch.no_grad():
        SR_1_4, SR_1_2, output = model_large(ms, pan)
    print(f"  å‚æ•°é‡: {large_params:,}")
    print(f"  è¾“å‡ºå°ºå¯¸: {output.shape}")
    print(f"  å‚æ•°å¢å¹…: {((large_params - base_params) / base_params * 100):+.1f}%")

    print("\nMambaIRv2-GPPNN åˆå§‹åŒ–å®Œæˆï¼")
