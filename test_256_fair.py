# -*- coding: utf-8 -*-
"""
å…¬å¹³æµ‹è¯•è„šæœ¬ - 256å¯¹256
éªŒè¯MambaIRv2-GPPNNæ¶æ„åœ¨åŒåˆ†è¾¨ç‡ä¸‹çš„çœŸå®æ€§èƒ½
"""

import os
import torch
import torch.nn.functional as F
import cv2
import numpy as np
from PIL import Image
import argparse
from tqdm import tqdm
import time
import json

# æ·»åŠ æ¨¡å‹è·¯å¾„
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from models import create_mambairv2_gppnn


def calculate_psnr(img1, img2):
    """è®¡ç®—PSNR"""
    mse = torch.mean((img1 - img2) ** 2)
    if mse == 0:
        return float('inf')
    return 20 * torch.log10(1.0 / torch.sqrt(mse))


def calculate_ssim_simple(img1, img2):
    """ç®€åŒ–SSIMè®¡ç®—"""
    if isinstance(img1, torch.Tensor):
        img1 = img1.cpu().numpy()
    if isinstance(img2, torch.Tensor):
        img2 = img2.cpu().numpy()
    
    if len(img1.shape) == 4:
        img1 = img1[0]
        img2 = img2[0]
    
    # è½¬æ¢ä¸ºç°åº¦å›¾
    if img1.shape[0] == 3:
        img1 = np.mean(img1, axis=0)
        img2 = np.mean(img2, axis=0)
    
    mu1 = np.mean(img1)
    mu2 = np.mean(img2)
    sigma1 = np.var(img1)
    sigma2 = np.var(img2)
    sigma12 = np.mean((img1 - mu1) * (img2 - mu2))
    
    c1 = 0.01 ** 2
    c2 = 0.03 ** 2
    
    ssim_val = ((2 * mu1 * mu2 + c1) * (2 * sigma12 + c2)) / \
               ((mu1 ** 2 + mu2 ** 2 + c1) * (sigma1 + sigma2 + c2))
    
    return max(0, min(1, ssim_val))


def load_and_resize_image(img_path, target_size=(256, 256), is_grayscale=False):
    """åŠ è½½å¹¶resizeå›¾åƒåˆ°æŒ‡å®šå°ºå¯¸"""
    if is_grayscale:
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        img = cv2.resize(img, target_size)
        img = img.astype(np.float32) / 255.0
        return torch.from_numpy(img).unsqueeze(0)  # [1, H, W]
    else:
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, target_size)
        img = img.astype(np.float32) / 255.0
        return torch.from_numpy(img.transpose(2, 0, 1))  # [3, H, W]


def test_fair_256(model_path, test_dir, output_dir, device):
    """å…¬å¹³çš„256Ã—256æµ‹è¯• - å¢å¼ºç‰ˆï¼ˆåŒ…å«è¯¦ç»†æŒ‡æ ‡ï¼‰"""

    print(f"ğŸ§ª å¼€å§‹å…¬å¹³æµ‹è¯• - 256Ã—256å¯¹256Ã—256 (å¢å¼ºç‰ˆ)")
    print("="*70)
    print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"   æµ‹è¯•ç›®å½•: {test_dir}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   è®¾å¤‡: {device}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # åŠ è½½æ¨¡å‹
    print("\nğŸ—ï¸ åŠ è½½æ¨¡å‹...")
    checkpoint = torch.load(model_path, map_location=device)
    model_size = checkpoint.get('config', {}).get('model_size', 'base')
    print(f"   æ¨¡å‹å¤§å°: {model_size}")

    model = create_mambairv2_gppnn(model_size).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    print(f"   æœ€ä½³PSNR: {checkpoint.get('best_psnr', 'æœªçŸ¥'):.2f}dB")
    print(f"   è®­ç»ƒè½®æ•°: {checkpoint.get('epoch', 'æœªçŸ¥')}")

    # ğŸ”¥ æ–°å¢ï¼šåŠ è½½æŸå¤±å‡½æ•°è®¡ç®—å™¨
    from train import IRDN_Loss
    criterion = IRDN_Loss(alpha=1.0, beta=0.3, gamma=0.2, edge_aware=True, freq_loss=True).to(device)
    
    # è·å–æµ‹è¯•å›¾åƒåˆ—è¡¨
    gt_dir = os.path.join(test_dir, 'GT')
    ms_dir = os.path.join(test_dir, 'MS') 
    pan_dir = os.path.join(test_dir, 'PAN')
    
    if not all(os.path.exists(d) for d in [gt_dir, ms_dir, pan_dir]):
        raise FileNotFoundError(f"æµ‹è¯•ç›®å½•ä¸å®Œæ•´: {test_dir}")
    
    img_names = [f for f in os.listdir(gt_dir) if f.lower().endswith(('.jpg', '.png'))]
    img_names = sorted(img_names)
    print(f"\nğŸ“Š æ‰¾åˆ°æµ‹è¯•å›¾åƒ: {len(img_names)}å¼ ")
    
    # ğŸ”¥ å¢å¼ºçš„æµ‹è¯•ç»Ÿè®¡
    total_psnr = 0.0
    total_ssim = 0.0
    total_loss = 0.0
    total_l1_loss = 0.0
    total_grad_loss = 0.0
    total_ssim_loss = 0.0
    total_edge_loss = 0.0
    total_freq_loss = 0.0
    total_mse = 0.0

    psnr_list = []
    ssim_list = []
    loss_list = []

    valid_count = 0

    print(f"\nğŸ”¬ å¼€å§‹é€å¼ æµ‹è¯•...")
    start_time = time.time()
    
    with torch.no_grad():
        for i, img_name in enumerate(tqdm(img_names, desc="æµ‹è¯•è¿›åº¦")):
            try:
                # åŠ è½½åŸå§‹å›¾åƒå¹¶resizeåˆ°256Ã—256
                gt_path = os.path.join(gt_dir, img_name)
                ms_path = os.path.join(ms_dir, img_name)
                pan_path = os.path.join(pan_dir, img_name)
                
                # è¯»å–å¹¶resizeä¸º256Ã—256 (å…³é”®æ­¥éª¤!)
                gt_256 = load_and_resize_image(gt_path, (256, 256), False)
                ms_256 = load_and_resize_image(ms_path, (256, 256), False) 
                pan_256 = load_and_resize_image(pan_path, (256, 256), True)
                
                # æ·»åŠ batchç»´åº¦å¹¶ç§»åˆ°GPU
                gt_256 = gt_256.unsqueeze(0).to(device)    # [1, 3, 256, 256]
                ms_256 = ms_256.unsqueeze(0).to(device)    # [1, 3, 256, 256]
                pan_256 = pan_256.unsqueeze(0).to(device)  # [1, 1, 256, 256]
                
                # æ¨¡å‹æ¨ç†
                outputs = model(ms_256, pan_256)
                _, _, output_full = outputs

                # ğŸ”¥ è®¡ç®—è¯¦ç»†æŒ‡æ ‡ (256Ã—256 vs 256Ã—256 - å®Œå…¨å…¬å¹³!)
                psnr = calculate_psnr(output_full, gt_256)
                ssim = calculate_ssim_simple(output_full, gt_256)

                # ğŸ”¥ æ–°å¢ï¼šè®¡ç®—å„é¡¹æŸå¤±
                loss_dict = criterion(outputs, gt_256)
                total_loss_val = loss_dict['total_loss'].item()
                l1_loss_val = loss_dict['l1_loss'].item()
                grad_loss_val = loss_dict['grad_loss'].item()
                ssim_loss_val = loss_dict['ssim_loss'].item()
                edge_loss_val = loss_dict['edge_loss'].item()
                freq_loss_val = loss_dict['freq_loss'].item()

                # ğŸ”¥ æ–°å¢ï¼šè®¡ç®—MSE
                mse = torch.mean((output_full - gt_256) ** 2).item()

                # ç´¯åŠ ç»Ÿè®¡
                total_psnr += psnr.item()
                total_ssim += ssim
                total_loss += total_loss_val
                total_l1_loss += l1_loss_val
                total_grad_loss += grad_loss_val
                total_ssim_loss += ssim_loss_val
                total_edge_loss += edge_loss_val
                total_freq_loss += freq_loss_val
                total_mse += mse

                # ä¿å­˜å•å¼ æ•°æ®ï¼ˆç”¨äºåˆ†ææ–¹å·®ï¼‰
                psnr_list.append(psnr.item())
                ssim_list.append(ssim)
                loss_list.append(total_loss_val)

                valid_count += 1
                
                # ä¿å­˜ç»“æœå›¾åƒ (åªä¿å­˜å‰6å¼ )
                if i < 6:
                    # è½¬æ¢ä¸ºnumpyä¿å­˜
                    output_np = output_full[0].cpu().numpy().transpose(1, 2, 0)
                    output_np = np.clip(output_np * 255, 0, 255).astype(np.uint8)
                    output_pil = Image.fromarray(output_np)
                    
                    # ä¿å­˜ç»“æœ
                    result_name = f"result_{i:03d}_{img_name}"
                    output_pil.save(os.path.join(output_dir, result_name))
                    
                    # ä¹Ÿä¿å­˜GTå¯¹æ¯”
                    gt_np = gt_256[0].cpu().numpy().transpose(1, 2, 0)
                    gt_np = np.clip(gt_np * 255, 0, 255).astype(np.uint8)
                    gt_pil = Image.fromarray(gt_np)
                    gt_name = f"gt_{i:03d}_{img_name}"
                    gt_pil.save(os.path.join(output_dir, gt_name))
                
                # ğŸ”¥ å¢å¼ºçš„å®æ—¶æ˜¾ç¤ºè¿›åº¦
                if i % 20 == 0 and i > 0:
                    avg_psnr = total_psnr / valid_count
                    avg_ssim = total_ssim / valid_count
                    avg_loss = total_loss / valid_count
                    avg_mse = total_mse / valid_count
                    print(f"\n   ğŸ“Š ä¸­é—´ç»“æœ ({i}/{len(img_names)}å¼ ):")
                    print(f"      PSNR={avg_psnr:.2f}dB, SSIM={avg_ssim:.4f}, Loss={avg_loss:.6f}, MSE={avg_mse:.6f}")
                    
            except Exception as e:
                print(f"\nâŒ å¤„ç†{img_name}æ—¶å‡ºé”™: {e}")
                continue
    
    # æœ€ç»ˆç»Ÿè®¡
    test_time = time.time() - start_time
    
    if valid_count > 0:
        # è®¡ç®—å¹³å‡å€¼
        avg_psnr = total_psnr / valid_count
        avg_ssim = total_ssim / valid_count
        avg_loss = total_loss / valid_count
        avg_l1_loss = total_l1_loss / valid_count
        avg_grad_loss = total_grad_loss / valid_count
        avg_ssim_loss = total_ssim_loss / valid_count
        avg_edge_loss = total_edge_loss / valid_count
        avg_freq_loss = total_freq_loss / valid_count
        avg_mse = total_mse / valid_count

        # ğŸ”¥ è®¡ç®—æ ‡å‡†å·®ï¼ˆè¡¡é‡ç¨³å®šæ€§ï¼‰
        psnr_std = np.std(psnr_list)
        ssim_std = np.std(ssim_list)
        loss_std = np.std(loss_list)
        psnr_min = np.min(psnr_list)
        psnr_max = np.max(psnr_list)
        ssim_min = np.min(ssim_list)
        ssim_max = np.max(ssim_list)

        print(f"\nğŸ‰ å…¬å¹³æµ‹è¯•å®Œæˆ!")
        print("="*70)
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ (256Ã—256å¯¹256Ã—256) - å¢å¼ºç‰ˆ:")
        print(f"   æµ‹è¯•å›¾åƒ: {valid_count}/{len(img_names)}å¼ ")
        print(f"\nğŸ“ˆ ä¸»è¦æŒ‡æ ‡:")
        print(f"   å¹³å‡PSNR: {avg_psnr:.4f}dB  (èŒƒå›´: {psnr_min:.2f} ~ {psnr_max:.2f}dB, æ ‡å‡†å·®: {psnr_std:.3f})")
        print(f"   å¹³å‡SSIM: {avg_ssim:.4f}    (èŒƒå›´: {ssim_min:.4f} ~ {ssim_max:.4f}, æ ‡å‡†å·®: {ssim_std:.4f})")
        print(f"   å¹³å‡MSE:  {avg_mse:.6f}")
        print(f"\nğŸ”¬ æŸå¤±å‡½æ•°è¯¦ç»†åˆ†è§£:")
        print(f"   æ€»æŸå¤± (Total Loss):     {avg_loss:.6f}  (æ ‡å‡†å·®: {loss_std:.6f})")
        print(f"   â”œâ”€ L1æŸå¤±:              {avg_l1_loss:.6f}")
        print(f"   â”œâ”€ æ¢¯åº¦æŸå¤± (Ã—0.3):     {avg_grad_loss:.6f}")
        print(f"   â”œâ”€ SSIMæŸå¤± (Ã—0.2):     {avg_ssim_loss:.6f}")
        print(f"   â”œâ”€ è¾¹ç¼˜æŸå¤± (Ã—0.15):    {avg_edge_loss:.6f}")
        print(f"   â””â”€ é¢‘åŸŸæŸå¤± (Ã—0.1):     {avg_freq_loss:.6f}")
        print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
        print(f"   æµ‹è¯•è€—æ—¶: {test_time:.1f}ç§’")
        print(f"   å¹³å‡æ¯å¼ : {test_time/valid_count:.2f}ç§’")
        
        # æ€§èƒ½è¯„ä»·
        print(f"\nğŸ’¡ æ€§èƒ½è¯„ä»·:")
        if avg_psnr >= 30:
            print(f"   PSNR {avg_psnr:.2f}dB - ğŸŒŸ ä¼˜ç§€!")
        elif avg_psnr >= 28:
            print(f"   PSNR {avg_psnr:.2f}dB - âœ… è‰¯å¥½!")
        elif avg_psnr >= 25:
            print(f"   PSNR {avg_psnr:.2f}dB - âš ï¸  ä¸€èˆ¬")
        else:
            print(f"   PSNR {avg_psnr:.2f}dB - âŒ éœ€è¦æ”¹è¿›")
            
        if avg_ssim >= 0.95:
            print(f"   SSIM {avg_ssim:.4f} - ğŸŒŸ ä¼˜ç§€!")
        elif avg_ssim >= 0.90:
            print(f"   SSIM {avg_ssim:.4f} - âœ… è‰¯å¥½!")
        else:
            print(f"   SSIM {avg_ssim:.4f} - âš ï¸  éœ€è¦æ”¹è¿›")
        
        # ğŸ”¥ ä¿å­˜å¢å¼ºçš„æµ‹è¯•æŠ¥å‘Š
        report = {
            'model_path': model_path,
            'test_images': valid_count,
            'resolution': '256x256_fair_test',

            # ä¸»è¦æŒ‡æ ‡
            'metrics': {
                'psnr': {
                    'mean': float(avg_psnr),
                    'std': float(psnr_std),
                    'min': float(psnr_min),
                    'max': float(psnr_max)
                },
                'ssim': {
                    'mean': float(avg_ssim),
                    'std': float(ssim_std),
                    'min': float(ssim_min),
                    'max': float(ssim_max)
                },
                'mse': float(avg_mse)
            },

            # æŸå¤±å‡½æ•°åˆ†è§£
            'loss_breakdown': {
                'total_loss': {
                    'mean': float(avg_loss),
                    'std': float(loss_std)
                },
                'l1_loss': float(avg_l1_loss),
                'grad_loss': float(avg_grad_loss),
                'ssim_loss': float(avg_ssim_loss),
                'edge_loss': float(avg_edge_loss),
                'freq_loss': float(avg_freq_loss)
            },

            # æ€§èƒ½ç»Ÿè®¡
            'performance': {
                'test_time_sec': float(test_time),
                'time_per_image_sec': float(test_time/valid_count)
            }
        }

        with open(os.path.join(output_dir, 'test_report_256_fair.json'), 'w') as f:
            json.dump(report, f, indent=4)

        return avg_psnr, avg_ssim
    else:
        print("\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å›¾åƒ!")
        return 0, 0


def main():
    parser = argparse.ArgumentParser(description='å…¬å¹³æµ‹è¯• - 256Ã—256å¯¹256Ã—256')
    parser.add_argument('--model_path', type=str, 
                       default='checkpoints/mambairv2_gppnn_0925_1327/models/best_model.pth',
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--test_dir', type=str, 
                       default='photo/testdateset',
                       help='æµ‹è¯•æ•°æ®ç›®å½•')
    parser.add_argument('--output_dir', type=str,
                       default='test_results_256_fair',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='auto',
                       help='è®¾å¤‡ (auto/cuda/cpu)')
    
    args = parser.parse_args()
    
    # è®¾å¤‡è®¾ç½®
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    # å¼€å§‹æµ‹è¯•
    test_fair_256(args.model_path, args.test_dir, args.output_dir, device)


if __name__ == '__main__':
    main()
