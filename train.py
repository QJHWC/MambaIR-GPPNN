# -*- coding: utf-8 -*-
"""
MambaIR-GPPNN è®­ç»ƒè„šæœ¬
åŸºäºMambaIRv2çš„æœ€ä¼˜å…¨è‰²é”åŒ–ç½‘ç»œè®­ç»ƒ

è¿è¡Œæ–¹å¼:
python train.py --model_size base --batch_size 4 --epochs 50
python train.py --model_size large --batch_size 2 --epochs 100
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm
import time
import gc  # åƒåœ¾å›æ”¶
import glob  # æ–‡ä»¶åŒ¹é…
import psutil  # ç³»ç»Ÿç›‘æ§
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models import create_mambairv2_gppnn
from data import create_photo_dataloaders


class IRDN_Loss(nn.Module):
    """
    ğŸ”¥ å¢å¼ºçš„æŸå¤±å‡½æ•° - ä¼˜åŒ–38 (v2.2ä¼˜åŒ–ç‰ˆ)
    L1 + Gradient + SSIM + Edge-aware + Frequency
    v2.2: å¤§å¹…æå‡ç»“æ„æ„ŸçŸ¥æƒé‡ï¼Œå¼ºåŒ–SSIMå’Œè¾¹ç¼˜ä¿çœŸ
    """
    def __init__(self, alpha=1.0, beta=0.3, gamma=0.2, edge_aware=True, freq_loss=True, **kwargs):
        super().__init__()
        self.alpha = alpha          # L1æŸå¤±æƒé‡
        self.beta = beta            # æ¢¯åº¦æŸå¤±æƒé‡ï¼ˆ0.15â†’0.3ï¼ŒÃ—2å€å¼ºåŒ–ç»“æ„æ„ŸçŸ¥ï¼‰
        self.gamma = gamma          # SSIMæŸå¤±æƒé‡ï¼ˆ0.05â†’0.2ï¼ŒÃ—4å€ç›´æ¥ä¼˜åŒ–SSIMï¼‰
        self.edge_aware = edge_aware
        self.freq_loss = freq_loss

        self.l1_loss = nn.L1Loss()
        self.mse_loss = nn.MSELoss()
        
        # ğŸŒ ä¸–ç•Œæ¨¡å‹å¢å¼º: DSCç‰©ç†ä¸€è‡´æ€§æŸå¤±
        self.use_dsc = kwargs.get('use_dsc', False)
        if self.use_dsc:
            from models.world_model import SensorConsistencyLoss
            self.dsc_loss_fn = SensorConsistencyLoss(
                spectral_response=kwargs.get('dsc_spectral_response', [0.299, 0.587, 0.114]),
                mtf_kernel_size=kwargs.get('dsc_mtf_kernel_size', 5),
                mtf_sigma=kwargs.get('dsc_mtf_sigma', 1.0),
                lrms_weight=kwargs.get('dsc_lrms_weight', 0.3)
            )
            self.lambda_s = kwargs.get('lambda_s', 0.3)
            print(f"[Loss] [WorldModel] DSC (Sensor Consistency) enabled, lambda_s={self.lambda_s}")
        else:
            self.dsc_loss_fn = None
        
        # ğŸŒ ä¸–ç•Œæ¨¡å‹å¢å¼º: WAC-Xé¢‘åŸŸä¸€è‡´æ€§æŸå¤±
        self.use_wacx = kwargs.get('use_wacx', False)
        if self.use_wacx:
            from models.world_model import WACXLoss
            self.wacx_loss_fn = WACXLoss(
                interband_weight=kwargs.get('wacx_interband_weight', 1.0),
                pan_gate_weight=kwargs.get('wacx_pan_gate_weight', 0.5),
                freq_threshold=kwargs.get('wacx_freq_threshold', 0.1)
            )
            self.lambda_w = kwargs.get('lambda_w', 0.5)
            print(f"[Loss] [WorldModel] WAC-X (Cross-band Consistency) enabled, lambda_w={self.lambda_w}")
        else:
            self.wacx_loss_fn = None

    def gradient_loss(self, pred, target):
        """è®¡ç®—æ¢¯åº¦æŸå¤±"""
        pred_h = pred[:, :, 1:, :] - pred[:, :, :-1, :]
        pred_w = pred[:, :, :, 1:] - pred[:, :, :, :-1]
        target_h = target[:, :, 1:, :] - target[:, :, :-1, :]
        target_w = target[:, :, :, 1:] - target[:, :, :, :-1]

        loss_h = self.l1_loss(pred_h, target_h)
        loss_w = self.l1_loss(pred_w, target_w)

        return loss_h + loss_w

    def ssim_loss(self, pred, target):
        """ç®€åŒ–SSIMæŸå¤±"""
        c1 = 0.01 ** 2
        c2 = 0.03 ** 2

        mu_pred = torch.mean(pred, dim=[2, 3], keepdim=True)
        mu_target = torch.mean(target, dim=[2, 3], keepdim=True)

        sigma_pred = torch.var(pred, dim=[2, 3], keepdim=True)
        sigma_target = torch.var(target, dim=[2, 3], keepdim=True)
        sigma_pred_target = torch.mean((pred - mu_pred) * (target - mu_target), dim=[2, 3], keepdim=True)

        ssim = ((2 * mu_pred * mu_target + c1) * (2 * sigma_pred_target + c2)) / \
               ((mu_pred ** 2 + mu_target ** 2 + c1) * (sigma_pred + sigma_target + c2))

        return 1 - torch.mean(ssim)

    def edge_aware_loss(self, pred, target):
        """è¾¹ç¼˜æ„ŸçŸ¥æŸå¤±"""
        # Sobelç®—å­æ£€æµ‹è¾¹ç¼˜
        sobel_x = torch.tensor([[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]],
                               dtype=pred.dtype, device=pred.device).repeat(pred.shape[1], 1, 1, 1)
        sobel_y = torch.tensor([[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]],
                               dtype=pred.dtype, device=pred.device).repeat(pred.shape[1], 1, 1, 1)

        pred_edge_x = F.conv2d(pred, sobel_x, padding=1, groups=pred.shape[1])
        pred_edge_y = F.conv2d(pred, sobel_y, padding=1, groups=pred.shape[1])
        target_edge_x = F.conv2d(target, sobel_x, padding=1, groups=target.shape[1])
        target_edge_y = F.conv2d(target, sobel_y, padding=1, groups=target.shape[1])

        edge_loss = self.l1_loss(pred_edge_x, target_edge_x) + self.l1_loss(pred_edge_y, target_edge_y)
        return edge_loss

    def frequency_loss(self, pred, target):
        """é¢‘åŸŸæŸå¤±"""
        pred_fft = torch.fft.rfft2(pred, norm='ortho')
        target_fft = torch.fft.rfft2(target, norm='ortho')

        freq_loss = self.l1_loss(torch.abs(pred_fft), torch.abs(target_fft))
        return freq_loss
    
    def forward(self, outputs, target, **kwargs):
        """
        ğŸ”¥ å¢å¼ºçš„æŸå¤±è®¡ç®— - ä¼˜åŒ–39 + ä¸–ç•Œæ¨¡å‹å¢å¼º
        Args:
            outputs: [SR_1_4, SR_1_2, output] - æ¨¡å‹è¾“å‡º
            target: [B, C, H, W] - ç›®æ ‡GTå›¾åƒ
            **kwargs: é¢å¤–å‚æ•°ï¼ˆä¸–ç•Œæ¨¡å‹éœ€è¦ï¼‰
                - pan_gt: [B, 1, H, W] PAN Ground Truth (for DSC)
                - ms_gt: [B, C, H/4, W/4] MS Ground Truth (for DSC)
        """
        SR_1_4, SR_1_2, output_full = outputs

        # åˆ›å»ºå¤šå°ºåº¦ç›®æ ‡
        target_1_2 = nn.functional.avg_pool2d(target, 2, 2)
        target_1_4 = nn.functional.avg_pool2d(target_1_2, 2, 2)

        # L1æŸå¤±
        l1_loss_1_4 = self.l1_loss(SR_1_4[0], target_1_4)
        l1_loss_1_2 = self.l1_loss(SR_1_2[0], target_1_2)
        l1_loss_full = self.l1_loss(output_full, target)
        total_l1 = l1_loss_1_4 + l1_loss_1_2 + l1_loss_full

        # æ¢¯åº¦æŸå¤±ï¼ˆä»…åœ¨å…¨åˆ†è¾¨ç‡ä¸Šè®¡ç®—ï¼‰
        grad_loss = self.gradient_loss(output_full, target)

        # ğŸ”¥ æ–°å¢: SSIMæŸå¤±
        ssim_loss_val = self.ssim_loss(output_full, target) if self.gamma > 0 else 0

        # ğŸ”¥ æ–°å¢: è¾¹ç¼˜æ„ŸçŸ¥æŸå¤± (v2.2: 0.1â†’0.15)
        edge_loss_val = self.edge_aware_loss(output_full, target) * 0.15 if self.edge_aware else 0

        # ğŸ”¥ æ–°å¢: é¢‘åŸŸæŸå¤± (v2.2: 0.05â†’0.1)
        freq_loss_val = self.frequency_loss(output_full, target) * 0.1 if self.freq_loss else 0

        # ğŸŒ ä¸–ç•Œæ¨¡å‹å¢å¼º: DSCç‰©ç†ä¸€è‡´æ€§æŸå¤±
        dsc_loss_val = torch.tensor(0.0, device=total_loss.device)
        if self.use_dsc and self.dsc_loss_fn is not None:
            # éœ€è¦ä»kwargsè·å–pan_gtå’Œms_gt
            if 'pan_gt' in kwargs and 'ms_gt' in kwargs:
                dsc_losses = self.dsc_loss_fn(
                    hrms_pred=output_full,
                    pan_gt=kwargs['pan_gt'],
                    lrms_gt=kwargs.get('ms_gt', None)
                )
                dsc_loss_val = dsc_losses['dsc_total']

        # ğŸŒ ä¸–ç•Œæ¨¡å‹å¢å¼º: WAC-Xé¢‘åŸŸä¸€è‡´æ€§æŸå¤±
        wacx_loss_val = torch.tensor(0.0, device=total_loss.device)
        if self.use_wacx and self.wacx_loss_fn is not None:
            if 'pan_gt' in kwargs:
                wacx_losses = self.wacx_loss_fn(
                    hrms=output_full,
                    pan=kwargs['pan_gt']
                )
                wacx_loss_val = wacx_losses['wacx_total']

        # æ€»æŸå¤±
        total_loss = (self.alpha * total_l1 +
                     self.beta * grad_loss +
                     self.gamma * ssim_loss_val +
                     edge_loss_val +
                     freq_loss_val +
                     (self.lambda_s * dsc_loss_val if self.use_dsc else 0.0) +
                     (self.lambda_w * wacx_loss_val if self.use_wacx else 0.0))

        return {
            'total_loss': total_loss,
            'l1_loss': total_l1,
            'grad_loss': grad_loss,
            'ssim_loss': ssim_loss_val if isinstance(ssim_loss_val, torch.Tensor) else torch.tensor(0.0, device=total_loss.device),
            'edge_loss': edge_loss_val if isinstance(edge_loss_val, torch.Tensor) else torch.tensor(0.0, device=total_loss.device),
            'freq_loss': freq_loss_val if isinstance(freq_loss_val, torch.Tensor) else torch.tensor(0.0, device=total_loss.device),
            'dsc_loss': dsc_loss_val,  # ğŸŒ æ–°å¢DSCæŸå¤±
            'wacx_loss': wacx_loss_val,  # ğŸŒ æ–°å¢WAC-XæŸå¤±
            'l1_1_4': l1_loss_1_4,
            'l1_1_2': l1_loss_1_2,
            'l1_full': l1_loss_full
        }


class ModelEMA:
    """
    ğŸ”¥ EMA (Exponential Moving Average) æ¨¡å‹å¹³æ»‘ - v2.2æ–°å¢
    æå‡éªŒè¯ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›
    """
    def __init__(self, model, decay=0.9999, device=None):
        self.ema = {k: v.clone().detach() for k, v in model.state_dict().items()}
        self.decay = decay
        self.device = device
        if self.device is not None:
            self.ema = {k: v.to(device) for k, v in self.ema.items()}

    def update(self, model):
        """æ›´æ–°EMAæƒé‡"""
        with torch.no_grad():
            for k, v in model.state_dict().items():
                if k in self.ema:
                    self.ema[k] = self.decay * self.ema[k] + (1 - self.decay) * v

    def apply_shadow(self, model):
        """å°†EMAæƒé‡åº”ç”¨åˆ°æ¨¡å‹"""
        with torch.no_grad():
            model.load_state_dict(self.ema)

    def store(self, model):
        """ä¿å­˜å½“å‰æ¨¡å‹æƒé‡ï¼ˆç”¨äºæ¢å¤ï¼‰"""
        self.backup = {k: v.clone().detach() for k, v in model.state_dict().items()}

    def restore(self, model):
        """æ¢å¤ä¿å­˜çš„æ¨¡å‹æƒé‡"""
        if hasattr(self, 'backup'):
            model.load_state_dict(self.backup)


def calculate_psnr(img1, img2):
    """è®¡ç®—PSNR"""
    mse = torch.mean((img1 - img2) ** 2)
    if mse == 0:
        return float('inf')
    return 20 * torch.log10(1.0 / torch.sqrt(mse))


def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, args, writer=None, ema=None, scaler=None):
    """è®­ç»ƒä¸€ä¸ªepoch - å¸¦æ˜¾å­˜ä¿æŠ¤ + EMAæ›´æ–° + æ··åˆç²¾åº¦ (v2.2)"""
    model.train()
    total_loss = 0.0
    total_psnr = 0.0
    num_batches = 0
    global_step = epoch * len(train_loader)
    
    # ğŸ”¥ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    grad_accum_steps = getattr(args, 'grad_accum_steps', 1)
    accum_step = 0

    epoch_start = time.time()
    
    for batch_idx, (ms, pan, gt) in enumerate(train_loader):
        try:
            ms, pan, gt = ms.to(device), pan.to(device), gt.to(device)
            
            # ğŸ”¥ æ¢¯åº¦ç´¯ç§¯ï¼šç¬¬ä¸€æ­¥å‰æ¸…ç©ºæ¢¯åº¦
            if accum_step == 0:
                optimizer.zero_grad()
            
            # ğŸ”¥ æ··åˆç²¾åº¦è®­ç»ƒ + æ¢¯åº¦ç´¯ç§¯
            if scaler is not None:
                # FP16æ··åˆç²¾åº¦è·¯å¾„
                with torch.cuda.amp.autocast():
                    # å‰å‘ä¼ æ’­
                    outputs = model(ms, pan)
                    
                    # è®¡ç®—æŸå¤±
                    loss_dict = criterion(outputs, gt, pan_gt=pan, ms_gt=ms)
                    loss = loss_dict['total_loss']
                    
                    # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±å½’ä¸€åŒ–
                    loss = loss / grad_accum_steps
                
                # åå‘ä¼ æ’­ï¼ˆscaledï¼‰
                scaler.scale(loss).backward()
                
                # æ¢¯åº¦ç´¯ç§¯ï¼šä»…åœ¨ç´¯ç§¯æ»¡æ—¶æ›´æ–°
                accum_step += 1
                if accum_step >= grad_accum_steps:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip_norm)
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
                    accum_step = 0
                    
                    # ğŸ”¥ EMAä»…åœ¨å‚æ•°æ›´æ–°åæ‰§è¡Œ
                    if ema is not None:
                        ema.update(model)
            else:
                # FP32æ ‡å‡†è·¯å¾„
                outputs = model(ms, pan)
                loss_dict = criterion(outputs, gt, pan_gt=pan, ms_gt=ms)
                loss = loss_dict['total_loss']
                loss = loss / grad_accum_steps
                
                loss.backward()
                
                accum_step += 1
                if accum_step >= grad_accum_steps:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip_norm)
                    optimizer.step()
                    optimizer.zero_grad()
                    accum_step = 0
                    
                    # ğŸ”¥ æ¢¯åº¦ç´¯ç§¯åæ›´æ–°EMA
                    if ema is not None:
                        ema.update(model)

            # ğŸ”¥ v2.2ä¿®å¤: è®°å½•è®­ç»ƒæ—¶çš„å³°å€¼GPUæ˜¾å­˜ (åœ¨åˆ é™¤tensorä¹‹å‰)
            if torch.cuda.is_available():
                peak_gpu_allocated = torch.cuda.memory_allocated() / 1024**3
                peak_gpu_reserved = torch.cuda.memory_reserved() / 1024**3
            else:
                peak_gpu_allocated = 0.0
                peak_gpu_reserved = 0.0

        except (RuntimeError, Exception) as e:
            print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹é”™è¯¯ (batch {batch_idx}): {e}")
            print(f"ğŸ”§ å°è¯•æ¸…ç†æ˜¾å­˜å¹¶ç»§ç»­...")
            
            # ç´§æ€¥æ¸…ç†å’Œé‡ç½®
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
                torch.cuda.synchronize()
            
            # é‡ç½®ä¼˜åŒ–å™¨æ¢¯åº¦ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
            optimizer.zero_grad()
            
            # å¦‚æœè¿ç»­é”™è¯¯è¶…è¿‡10æ¬¡ï¼Œåœæ­¢è®­ç»ƒ
            if not hasattr(train_one_epoch, 'error_count'):
                train_one_epoch.error_count = 0
            train_one_epoch.error_count += 1
            
            if train_one_epoch.error_count > 10:
                print(f"\nğŸ’¥ è¿ç»­é”™è¯¯è¶…è¿‡10æ¬¡ï¼Œåœæ­¢è®­ç»ƒé¿å…æ— é™å¾ªç¯")
                raise e
            
            # è·³è¿‡è¿™ä¸ªbatchï¼Œç»§ç»­è®­ç»ƒ
            continue  # ğŸ”¥ é‡è¦ï¼šè¿™é‡Œä¼šè·³è¿‡ä¸‹é¢æ‰€æœ‰ä»£ç 
        
        # ğŸ”§ æˆåŠŸæ‰§è¡Œbatchï¼Œé‡ç½®é”™è¯¯è®¡æ•°
        if hasattr(train_one_epoch, 'error_count'):
            train_one_epoch.error_count = 0
        
        # ğŸ”§ å®‰å…¨çš„ç»Ÿè®¡ä¿¡æ¯æå– (å…ˆä¿å­˜æ‰€æœ‰éœ€è¦çš„å€¼ï¼Œå†åˆ é™¤å˜é‡)
        # æ³¨æ„ï¼šæ¢¯åº¦ç´¯ç§¯æ—¶losså·²ç»é™¤ä»¥grad_accum_stepsï¼Œéœ€è¦Ã—å›æ¥æ˜¾ç¤ºçœŸå®å€¼
        loss_value = loss.item() * grad_accum_steps
        l1_loss_value = loss_dict['l1_loss'].item()
        grad_loss_value = loss_dict['grad_loss'].item()
        
        # å®‰å…¨çš„PSNRè®¡ç®— (ä¸æå‰åˆ é™¤tensorå¼•ç”¨)
        with torch.no_grad():
            _, _, output_full = outputs
            psnr_value = calculate_psnr(output_full, gt).item()
        
        total_loss += loss_value
        total_psnr += psnr_value
        num_batches += 1
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç«‹å³åˆ é™¤outputsé‡Šæ”¾æ˜¾å­˜ï¼ˆæ¢¯åº¦ç´¯ç§¯æ—¶å¾ˆé‡è¦ï¼ï¼‰
        del outputs, loss_dict, loss, output_full
        
        # ğŸ”¥ æ¢¯åº¦ç´¯ç§¯æ—¶çš„æ˜¾å­˜æ¸…ç†
        if grad_accum_steps > 1 and torch.cuda.is_available():
            # æ¯ä¸ªbatchåæ¸…ç†ï¼Œé˜²æ­¢ç´¯ç§¯
            torch.cuda.empty_cache()
        
        # æ·»åŠ æ˜¾å¼çš„CUDAåŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆ
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        
        # ğŸ”§ å®Œå…¨æ— å†²çªçš„æ¸…ç†ç­–ç•¥ - å®æ—¶æ˜¾å­˜ç›‘æ§
        # ä½¿ç”¨è´¨æ•°æ¥é¿å…ä¸æ—¥å¿—é¢‘ç‡(60)çš„æœ€å°å…¬å€æ•°å†²çª
        if batch_idx > 0 and batch_idx % 97 == 0:  # 97æ˜¯è´¨æ•°ï¼Œä¸60äº’è´¨ï¼Œé¿å…å†²çª
            try:
                if torch.cuda.is_available():
                    # æ¸…ç†å‰æŸ¥è¯¢
                    allocated_before = torch.cuda.memory_allocated() / 1024**3
                    reserved_before = torch.cuda.memory_reserved() / 1024**3
                    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3

                    # æ‰§è¡Œæ¸…ç†
                    torch.cuda.empty_cache()
                    gc.collect()
                    torch.cuda.synchronize()

                    # æ¸…ç†åæŸ¥è¯¢
                    allocated_after = torch.cuda.memory_allocated() / 1024**3
                    reserved_after = torch.cuda.memory_reserved() / 1024**3

                    print(f"\nğŸ§¹ å®šæœŸæ¸…ç† (batch {batch_idx}):")
                    print(f"   GPUæ€»æ˜¾å­˜: {total_memory:.1f}GB")
                    print(f"   æ¸…ç†å‰: å·²ç”¨{allocated_before:.1f}GB / ç¼“å­˜{reserved_before:.1f}GB")
                    print(f"   æ¸…ç†å: å·²ç”¨{allocated_after:.1f}GB / ç¼“å­˜{reserved_after:.1f}GB")
                    print(f"   é‡Šæ”¾: {reserved_before - reserved_after:.1f}GB")
            except Exception as e:
                print(f"\nâš ï¸  æ¸…ç†è¿‡ç¨‹å‡ºç°å¼‚å¸¸: {e}")

        # ç´§æ€¥æ¸…ç†ï¼šä½¿ç”¨å¦ä¸€ä¸ªè´¨æ•°é¿å…å†²çª
        elif batch_idx > 0 and batch_idx % 47 == 0 and torch.cuda.is_available():  # 47ä¹Ÿæ˜¯è´¨æ•°
            try:
                allocated = torch.cuda.memory_allocated() / 1024**3
                total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                # åªæœ‰åœ¨æ˜¾å­˜çœŸæ­£ç´§å¼ æ—¶æ‰æ¸…ç†
                if allocated > total_memory * 0.7:  # è¶…è¿‡70%æ‰æ¸…ç†
                    reserved_before = torch.cuda.memory_reserved() / 1024**3
                    print(f"\nğŸš¨ ç´§æ€¥æ¸…ç†æ˜¾å­˜: {allocated:.1f}GB / {total_memory:.1f}GB (ä½¿ç”¨ç‡{allocated/total_memory*100:.1f}%)")
                    torch.cuda.empty_cache()
                    gc.collect()
                    torch.cuda.synchronize()
                    reserved_after = torch.cuda.memory_reserved() / 1024**3
                    print(f"   é‡Šæ”¾ç¼“å­˜: {reserved_before - reserved_after:.1f}GB")
            except Exception as e:
                print(f"\nâš ï¸  ç´§æ€¥æ¸…ç†å¼‚å¸¸: {e}")
        
        # æ‰¹æ¬¡æ—¥å¿— (ä½¿ç”¨ä¿å­˜çš„å€¼ï¼Œé¿å…è®¿é—®å·²åˆ é™¤çš„å˜é‡)
        if batch_idx % args.log_freq == 0:
            # ğŸ”¥ v2.2ä¿®å¤: ä½¿ç”¨å³°å€¼GPUæ˜¾å­˜æ˜¾ç¤ºï¼ˆè®­ç»ƒæ—¶çš„çœŸå®å ç”¨ï¼‰
            if torch.cuda.is_available() and 'peak_gpu_allocated' in locals():
                gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                gpu_usage_pct = (peak_gpu_allocated / gpu_mem_total) * 100
                mem_info = f"GPU: {peak_gpu_allocated:.1f}/{gpu_mem_total:.1f}GB ({gpu_usage_pct:.0f}%) [å³°å€¼]"
            else:
                mem_info = ""

            print(f"Epoch [{epoch+1:3d}/{args.epochs}] "
                  f"Batch [{batch_idx:3d}/{len(train_loader)}] "
                  f"Loss: {loss_value:.6f} "
                  f"L1: {l1_loss_value:.6f} "
                  f"Grad: {grad_loss_value:.6f} "
                  f"PSNR: {psnr_value:.2f}dB  "
                  f"{mem_info}")
            
            # TensorBoardè®°å½• (ä½¿ç”¨ä¿å­˜çš„å€¼) - æ·»åŠ å¼‚å¸¸ä¿æŠ¤
            if writer is not None:
                try:
                    current_step = global_step + batch_idx
                    writer.add_scalar('Train/BatchLoss', loss_value, current_step)
                    writer.add_scalar('Train/BatchPSNR', psnr_value, current_step)
                    writer.add_scalar('Train/BatchL1', l1_loss_value, current_step)
                    writer.add_scalar('Train/BatchGrad', grad_loss_value, current_step)
                except Exception as e:
                    print(f"\nâš ï¸  TensorBoardå†™å…¥å¼‚å¸¸: {e}")
                    # ä¸ä¸­æ–­è®­ç»ƒï¼Œç»§ç»­è¿›è¡Œ
    
    # è®¡ç®—å¹³å‡å€¼å¹¶è¿”å› - æ·»åŠ é˜²æ­¢é™¤é›¶ä¿æŠ¤
    if num_batches > 0:
        avg_loss = total_loss / num_batches
        avg_psnr = total_psnr / num_batches
    else:
        print("\nâš ï¸  è­¦å‘Š: æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•batchï¼")
        avg_loss = float('inf')
        avg_psnr = 0.0
    
    epoch_time = time.time() - epoch_start
    
    # é‡ç½®é”™è¯¯è®¡æ•°å™¨
    if hasattr(train_one_epoch, 'error_count'):
        train_one_epoch.error_count = 0
    
    return avg_loss, avg_psnr, epoch_time


def calculate_ssim_simple(img1, img2):
    """ç®€åŒ–çš„SSIMè®¡ç®— (å‚è€ƒtrain_IRDN_migrated.py)"""
    # è½¬æ¢ä¸ºnumpy
    if isinstance(img1, torch.Tensor):
        img1 = img1.cpu().numpy()
    if isinstance(img2, torch.Tensor):
        img2 = img2.cpu().numpy()
    
    # å¦‚æœæ˜¯æ‰¹æ¬¡æ•°æ®ï¼Œå–ç¬¬ä¸€ä¸ª
    if len(img1.shape) == 4:
        img1 = img1[0]
        img2 = img2[0]
    
    # è½¬æ¢ä¸ºç°åº¦å›¾
    if img1.shape[0] == 3:  # RGB
        img1 = np.mean(img1, axis=0)
        img2 = np.mean(img2, axis=0)
    
    # ç®€åŒ–ç‰ˆSSIM
    mu1 = np.mean(img1)
    mu2 = np.mean(img2)
    sigma1 = np.var(img1)
    sigma2 = np.var(img2)
    sigma12 = np.mean((img1 - mu1) * (img2 - mu2))
    
    c1 = 0.01 ** 2
    c2 = 0.03 ** 2
    
    ssim_val = ((2 * mu1 * mu2 + c1) * (2 * sigma12 + c2)) / \
               ((mu1 ** 2 + mu2 ** 2 + c1) * (sigma1 + sigma2 + c2))
    
    return max(0, min(1, ssim_val))


def validate(model, val_loader, criterion, device):
    """éªŒè¯ - å¢åŠ SSIMæŒ‡æ ‡"""
    model.eval()
    total_loss = 0.0
    total_psnr = 0.0
    total_ssim = 0.0
    num_batches = 0
    
    with torch.no_grad():
        for ms, pan, gt in val_loader:
            ms, pan, gt = ms.to(device), pan.to(device), gt.to(device)
            
            outputs = model(ms, pan)
            loss_dict = criterion(outputs, gt)
            
            _, _, output_full = outputs
            psnr = calculate_psnr(output_full, gt)
            ssim = calculate_ssim_simple(output_full, gt)
            
            total_loss += loss_dict['total_loss'].item()
            total_psnr += psnr.item()
            total_ssim += ssim
            num_batches += 1
    
    avg_loss = total_loss / num_batches
    avg_psnr = total_psnr / num_batches
    avg_ssim = total_ssim / num_batches
    
    return avg_loss, avg_psnr, avg_ssim


def auto_find_max_batch_size(model, train_loader, criterion, optimizer, device, args):
    """
    ğŸ”¥ è‡ªåŠ¨æŸ¥æ‰¾æœ€å¤§å¯ç”¨batch_size
    ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ç­–ç•¥ï¼Œé¿å…OOMçš„åŒæ—¶æœ€å¤§åŒ–è®­ç»ƒé€Ÿåº¦
    """
    print("\n" + "="*70)
    print("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾æœ€å¤§å¯ç”¨ batch_size...")
    print("="*70)
    
    # ğŸ”§ æµ‹è¯•å‰å½»åº•æ¸…ç†æ˜¾å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()
    time.sleep(1)  # ç­‰å¾…æ¸…ç†å®Œæˆ

    # å€™é€‰batch_sizeåˆ—è¡¨ï¼ˆä»å¤§åˆ°å°æµ‹è¯•ï¼‰
    if args.img_size == 256:
        candidates = [32, 24, 20, 16, 12, 8, 6, 4, 2, 1]
    else:  # 512
        candidates = [8, 6, 4, 3, 2, 1]  # ğŸ”§ 512é™ä½èµ·å§‹å€¼

    max_working_bs = 1  # é»˜è®¤æœ€å°å€¼

    # è·å–ä¸€ä¸ªbatchçš„æ•°æ®ç”¨äºæµ‹è¯•
    try:
        ms_sample, pan_sample, gt_sample = next(iter(train_loader))
        original_batch_size = ms_sample.shape[0]
        print(f"ğŸ“¦ æ•°æ®åŠ è½½å™¨batch_size: {original_batch_size}")
    except Exception as e:
        print(f"âŒ æ— æ³•è·å–æµ‹è¯•æ•°æ®: {e}")
        return args.batch_size

    for test_bs in candidates:
        if test_bs > original_batch_size:
            continue  # è·³è¿‡è¶…è¿‡æ•°æ®åŠ è½½å™¨batch_sizeçš„å€¼

        print(f"\nğŸ§ª æµ‹è¯• batch_size={test_bs}...", end=" ")

        try:
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            if test_bs < original_batch_size:
                ms = ms_sample[:test_bs].to(device)
                pan = pan_sample[:test_bs].to(device)
                gt = gt_sample[:test_bs].to(device)
            else:
                ms = ms_sample.to(device)
                pan = pan_sample.to(device)
                gt = gt_sample.to(device)

            # æµ‹è¯•å‰å‘+åå‘ä¼ æ’­
            optimizer.zero_grad()
            outputs = model(ms, pan)
            # ğŸ”¥ ä¼ é€’ä¸–ç•Œæ¨¡å‹éœ€è¦çš„å‚æ•°
            loss_dict = criterion(outputs, gt, pan_gt=pan, ms_gt=ms)
            loss = loss_dict['total_loss']
            loss.backward()
            optimizer.step()

            # æŸ¥è¯¢æ˜¾å­˜å ç”¨
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**3
                total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
                usage_pct = (allocated / total_mem) * 100
                print(f"âœ… æˆåŠŸ! æ˜¾å­˜: {allocated:.1f}/{total_mem:.1f}GB ({usage_pct:.0f}%)")
            else:
                print("âœ… æˆåŠŸ!")

            max_working_bs = test_bs

            # æ¸…ç†æ˜¾å­˜
            del ms, pan, gt, outputs, loss_dict, loss
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            gc.collect()

            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæˆåŠŸçš„å°±åœæ­¢ï¼ˆä»å¤§åˆ°å°æµ‹è¯•ï¼‰
            break

        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"âŒ OOM")
                # æ¸…ç†æ˜¾å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                gc.collect()
                time.sleep(0.5)  # ç­‰å¾…æ¸…ç†å®Œæˆ
                continue
            else:
                print(f"âŒ é”™è¯¯: {e}")
                break
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
            break

    print("\n" + "="*70)
    print(f"ğŸ¯ è‡ªåŠ¨æ£€æµ‹ç»“æœ: æœ€å¤§å¯ç”¨ batch_size = {max_working_bs}")
    print("="*70)

    return max_working_bs


def main():
    parser = argparse.ArgumentParser(description='MambaIRv2-GPPNN Training')
    parser.add_argument('--model_size', type=str, default='base', choices=['base', 'large'],
                        help='Model size (base/large)')
    parser.add_argument('--batch_size', type=int, default=4, help='Batch size (ä¿å®ˆé…ç½®ï¼Œæ˜¾å­˜å®‰å…¨)')
    parser.add_argument('--epochs', type=int, default=190, help='Number of epochs (v2.2ä¼˜åŒ–: å¢åŠ è®­ç»ƒè½®æ•°)')
    parser.add_argument('--lr', type=float, default=0.0002, help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')
    parser.add_argument('--photo_root', type=str, default='./photo', help='Photo directory path')
    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Save directory')
    parser.add_argument('--log_dir', type=str, default='./logs', help='Log directory')
    parser.add_argument('--num_workers', type=int, default=4, help='Number of data workers')
    parser.add_argument('--img_size', type=int, default=256, help='Image size (256å¿«é€ŸéªŒè¯/512å®Œæ•´è®­ç»ƒ)')
    parser.add_argument('--save_freq', type=int, default=5, help='Save frequency (epochs)')
    parser.add_argument('--log_freq', type=int, default=10, help='Log frequency (batches)')
    parser.add_argument('--val_freq', type=int, default=5, help='Validation frequency (epochs, v2.2: 10â†’5æ›´é¢‘ç¹)')
    parser.add_argument('--grad_clip_norm', type=float, default=0.1, help='Gradient clipping norm')
    parser.add_argument('--device', type=str, default='auto', help='Device (auto/cuda/cpu)')
    parser.add_argument('--resume', type=str, default='', help='Resume training from checkpoint path')
    parser.add_argument('--auto_resume', action='store_true', help='Auto resume from latest checkpoint')
    parser.add_argument('--auto_batch_size', action='store_true', help='ğŸ”¥ è‡ªåŠ¨æŸ¥æ‰¾æœ€å¤§å¯ç”¨batch_sizeï¼ˆæ¨èï¼‰')
    
    # ğŸŒ ä¸–ç•Œæ¨¡å‹å¢å¼ºå‚æ•°
    parser.add_argument('--enable_world_model', action='store_true', help='å¯ç”¨ä¸–ç•Œæ¨¡å‹å¢å¼ºï¼ˆæ€»å¼€å…³ï¼‰')
    parser.add_argument('--use_wsm', action='store_true', help='å¯ç”¨ä¸–ç•ŒçŠ¶æ€è®°å¿†(WSM)')
    parser.add_argument('--use_dca_fim', action='store_true', help='å¯ç”¨å¯å½¢å˜å¯¹é½(DCA-FIM)')
    parser.add_argument('--use_dsc', action='store_true', help='å¯ç”¨ç‰©ç†ä¸€è‡´æ€§æŸå¤±(DSC)')
    parser.add_argument('--use_wacx', action='store_true', help='å¯ç”¨é¢‘åŸŸä¸€è‡´æ€§æŸå¤±(WAC-X)')
    parser.add_argument('--lambda_s', type=float, default=0.3, help='DSCæŸå¤±æƒé‡')
    parser.add_argument('--lambda_w', type=float, default=0.5, help='WAC-XæŸå¤±æƒé‡')
    parser.add_argument('--grad_accum_steps', type=int, default=1, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰')
    parser.add_argument('--fp16', action='store_true', help='ğŸ”¥ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼Œæ¨èV100/A100ï¼‰')

    args = parser.parse_args()
    
    # è®¾å¤‡è®¾ç½®
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒMambaIRv2-GPPNN ({args.model_size.upper()})")
    print("="*60)
    print(f"   è®¾å¤‡: {device}")
    print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size} (å®‰å…¨è®¾ç½®)")
    print(f"   è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"   å­¦ä¹ ç‡: {args.lr}")
    print(f"   å›¾åƒå°ºå¯¸: {args.img_size}x{args.img_size}")
    
    # ğŸš¨ å®‰å…¨æ£€æŸ¥ (å‚è€ƒtrain_IRDN_migrated.py)
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   GPUæ˜¾å­˜: {gpu_memory:.1f}GB")
        if gpu_memory < 6:
            print("   âš ï¸ æ˜¾å­˜è¾ƒå°ï¼Œä½¿ç”¨ä¿å®ˆé…ç½®")
    
    # ç³»ç»Ÿå†…å­˜æ£€æŸ¥
    system_memory = psutil.virtual_memory()
    print(f"   ç³»ç»Ÿå†…å­˜: {system_memory.available/1024**3:.1f}GB å¯ç”¨")
    if system_memory.percent > 80:
        print("   âš ï¸ ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å…³é—­å…¶ä»–ç¨‹åº")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%m%d_%H%M")
    save_dir = f"{args.save_dir}/mambairv2_gppnn_{timestamp}"
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs(f"{save_dir}/models", exist_ok=True)
    args.save_dir = save_dir  # æ›´æ–°ä¿å­˜ç›®å½•
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹...")
    
    # ğŸŒ ä¸–ç•Œæ¨¡å‹é…ç½®
    world_model_kwargs = {}
    if args.enable_world_model:
        world_model_kwargs.update({
            'use_wsm': args.use_wsm,
            'use_dca_fim': args.use_dca_fim,
            'wsm_hidden_dim': 128,
            'wsm_dropout': 0.1,
            'dca_num_points': 4,
            'dca_deform_weight': 0.3,
        })
        print(f"   ğŸŒ ä¸–ç•Œæ¨¡å‹å¢å¼ºå·²å¯ç”¨:")
        print(f"      WSM: {args.use_wsm}, DCA-FIM: {args.use_dca_fim}")
        print(f"      DSC: {args.use_dsc}, WAC-X: {args.use_wacx}")
    
    model = create_mambairv2_gppnn(args.model_size, **world_model_kwargs).to(device)
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   æ€»å‚æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader, test_loader = create_photo_dataloaders(
        photo_root=args.photo_root,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        img_size=(args.img_size, args.img_size)
    )
    
    # ğŸ”¥ ä¼˜åŒ–40: åˆ›å»ºå¢å¼ºçš„æŸå¤±å‡½æ•°å’ŒAdamWä¼˜åŒ–å™¨ (v2.2ä¼˜åŒ–ç‰ˆ)
    # ğŸŒ ä¸–ç•Œæ¨¡å‹æŸå¤±é…ç½®
    loss_kwargs = {}
    if args.enable_world_model:
        loss_kwargs.update({
            'use_dsc': args.use_dsc,
            'use_wacx': args.use_wacx,
            'lambda_s': args.lambda_s,
            'lambda_w': args.lambda_w,
        })
    
    criterion = IRDN_Loss(alpha=1.0, beta=0.3, gamma=0.2, edge_aware=True, freq_loss=True, **loss_kwargs)
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))

    # ğŸ”¥ æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰
    scaler = None
    if args.fp16:
        print("\nâš¡ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (FP16)...")
        scaler = torch.cuda.amp.GradScaler()
        print("   æ˜¾å­˜é¢„æœŸå‡å°‘: ~50%")
        print("   è®­ç»ƒé€Ÿåº¦æå‡: ~20%")

    # ğŸ”¥ v2.2æ–°å¢: EMAæ¨¡å‹å¹³æ»‘
    # ğŸ”§ 512Ã—512æ˜¾å­˜ä¼˜åŒ–ï¼šå¤§å°ºå¯¸å›¾åƒç¦ç”¨EMA
    if args.img_size >= 512:
        print("\nâš ï¸  [512æ¨¡å¼] ç¦ç”¨EMAèŠ‚çœæ˜¾å­˜...")
        ema = None
    else:
        print("\nğŸ”¥ åˆå§‹åŒ–EMAæ¨¡å‹å¹³æ»‘ (decay=0.9999)...")
        ema = ModelEMA(model, decay=0.9999, device=device)

    # ğŸ”¥ æ–°åŠŸèƒ½: è‡ªåŠ¨æŸ¥æ‰¾æœ€å¤§å¯ç”¨batch_size
    if args.auto_batch_size:
        print("\n" + "="*70)
        print("ğŸš€ å¯ç”¨è‡ªåŠ¨batch_sizeæ£€æµ‹...")
        print("="*70)

        # ä¿å­˜åŸå§‹batch_size
        original_bs = args.batch_size

        # è‡ªåŠ¨æ£€æµ‹
        optimal_bs = auto_find_max_batch_size(
            model, train_loader, criterion, optimizer, device, args
        )

        # å¦‚æœæ£€æµ‹åˆ°æ›´å¤§çš„batch_sizeï¼Œé‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨
        if optimal_bs > original_bs:
            print(f"\nğŸ”„ æ£€æµ‹åˆ°æ›´ä¼˜çš„batch_size={optimal_bs}ï¼Œé‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
            args.batch_size = optimal_bs
            train_loader, val_loader, test_loader = create_photo_dataloaders(
                photo_root=args.photo_root,
                batch_size=args.batch_size,
                num_workers=args.num_workers,
                img_size=(args.img_size, args.img_size)
            )
            print(f"âœ… æ•°æ®åŠ è½½å™¨å·²æ›´æ–°: batch_size={args.batch_size}")
        else:
            print(f"\nâœ… ä½¿ç”¨åŸå§‹batch_size={original_bs}")

    # ğŸ”¥ ä¼˜åŒ–41: Cosine Annealingå­¦ä¹ ç‡è°ƒåº¦å™¨ + Warmup (v2.2ä¼˜åŒ–ç‰ˆ)
    warmup_epochs = 8  # v2.2: ç»Ÿä¸€ä½¿ç”¨8è½®warmup
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=20, T_mult=2, eta_min=1e-7  # v2.2: æœ€å°å­¦ä¹ ç‡é™è‡³1e-7
    )

    # ğŸ”¥ v2.2æ–°å¢: Plateauæ£€æµ‹å™¨ - ä½œä¸ºå¤‡ç”¨
    plateau_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-7
    )
    
    # TensorBoard - æ·»åŠ å¼‚å¸¸ä¿æŠ¤
    try:
        writer = SummaryWriter(args.log_dir)
    except Exception as e:
        print(f"âš ï¸  TensorBoardåˆå§‹åŒ–å¤±è´¥: {e}")
        print("   å°†ç¦ç”¨TensorBoardæ—¥å¿—è®°å½•")
        writer = None
    
    # æ–­ç‚¹ç»­è®­å¤„ç†
    start_epoch = 0
    best_psnr = 0.0
    train_history = {'train_loss': [], 'train_psnr': [], 'val_loss': [], 'val_psnr': []}
    
    # æŸ¥æ‰¾æ–­ç‚¹æ–‡ä»¶
    resume_path = None
    if args.resume:
        resume_path = args.resume
        print(f"ğŸ”„ æŒ‡å®šæ–­ç‚¹ç»­è®­: {resume_path}")
    elif args.auto_resume:
        # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„checkpoint
        checkpoint_pattern = f"{args.save_dir}/mambairv2_gppnn_*/models/epoch_*.pth"
        checkpoints = glob.glob(checkpoint_pattern)
        if checkpoints:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
            latest_checkpoint = max(checkpoints, key=os.path.getmtime)
            resume_path = latest_checkpoint
            print(f"ğŸ”„ è‡ªåŠ¨æ‰¾åˆ°æœ€æ–°æ–­ç‚¹: {resume_path}")
    
    # åŠ è½½æ–­ç‚¹
    if resume_path and os.path.exists(resume_path):
        print(f"ğŸ“ åŠ è½½æ–­ç‚¹: {resume_path}")
        try:
            checkpoint = torch.load(resume_path, map_location=device)
            
            # åŠ è½½æ¨¡å‹çŠ¶æ€
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
            if 'scheduler_state_dict' in checkpoint:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            # åŠ è½½è®­ç»ƒä¿¡æ¯
            start_epoch = checkpoint['epoch'] + 1
            if 'best_psnr' in checkpoint:
                best_psnr = checkpoint['best_psnr']
            if 'train_history' in checkpoint:
                train_history = checkpoint['train_history']
            
            print(f"âœ… æ–­ç‚¹åŠ è½½æˆåŠŸ!")
            print(f"   ä»ç¬¬ {start_epoch+1} è½®ç»§ç»­è®­ç»ƒ")
            print(f"   å½“å‰æœ€ä½³PSNR: {best_psnr:.2f}dB")
            
        except Exception as e:
            print(f"âŒ æ–­ç‚¹åŠ è½½å¤±è´¥: {e}")
            print("   å°†ä»å¤´å¼€å§‹è®­ç»ƒ...")
            start_epoch = 0
            best_psnr = 0.0
    
    # è®­ç»ƒå¾ªç¯ (å‚è€ƒtrain_IRDN_migrated.pyæ ¼å¼)
    print(f"\nâš¡ å¼€å§‹è®­ç»ƒ...")
    print("="*60)
    start_time = time.time()
    
    for epoch in range(start_epoch, args.epochs):
        # è®­ç»ƒé˜¶æ®µ
        train_loss, train_psnr, epoch_time = train_one_epoch(
            model, train_loader, criterion, optimizer, device, epoch, args, writer, ema, scaler
        )
        
        train_history['train_loss'].append(train_loss)
        train_history['train_psnr'].append(train_psnr)
        
        print(f"\nğŸ“Š Epoch {epoch+1} è®­ç»ƒå®Œæˆ:")
        print(f"   å¹³å‡æŸå¤±: {train_loss:.6f}")
        print(f"   å¹³å‡PSNR: {train_psnr:.2f}dB")
        print(f"   è€—æ—¶: {epoch_time:.1f}s")
        
        # ğŸ§¹ Epochç»“æŸåæ¸…ç†æ˜¾å­˜ï¼Œé˜²æ­¢ç¢ç‰‡åŒ–
        torch.cuda.empty_cache()
        gc.collect()
        
        # æ·»åŠ trainlogé£æ ¼çš„SSIMè®°å½• (å‚è€ƒtrain_IRDN_migrated.py)
        print(f"The {epoch} Epoch mean-ssim is :{train_psnr/100:.6f}")  # ç®€åŒ–ç‰ˆæœ¬
        
        # éªŒè¯é˜¶æ®µ
        if (epoch + 1) % args.val_freq == 0:
            print(f"\nğŸ” éªŒè¯ Epoch {epoch+1}...")

            # ğŸ”¥ v2.2: ä½¿ç”¨EMAæ¨¡å‹è¿›è¡ŒéªŒè¯
            if ema is not None:
                ema.store(model)  # ä¿å­˜å½“å‰æƒé‡
                ema.apply_shadow(model)  # åº”ç”¨EMAæƒé‡

            val_loss, val_psnr, val_ssim = validate(model, val_loader, criterion, device)

            if ema is not None:
                ema.restore(model)  # æ¢å¤è®­ç»ƒæƒé‡

            train_history['val_loss'].append(val_loss)
            train_history['val_psnr'].append(val_psnr)

            print(f"   éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"   éªŒè¯PSNR: {val_psnr:.2f}dB (EMA)")
            print(f"   éªŒè¯SSIM: {val_ssim:.4f} (EMA)")
            
            # æ·»åŠ trainlogé£æ ¼çš„éªŒè¯è®°å½•
            print(f"The {epoch} Epoch validation mean-ssim is :{val_ssim:.6f}")
            
            # TensorBoardè®°å½• - æ·»åŠ å¼‚å¸¸ä¿æŠ¤
            if writer is not None:
                try:
                    writer.add_scalar('Train/Loss_Epoch', train_loss, epoch)
                    writer.add_scalar('Train/PSNR_Epoch', train_psnr, epoch)
                    writer.add_scalar('Val/Loss', val_loss, epoch)
                    writer.add_scalar('Val/PSNR', val_psnr, epoch)
                    writer.add_scalar('Val/SSIM', val_ssim, epoch)
                except Exception as e:
                    print(f"\nâš ï¸  TensorBoardéªŒè¯è®°å½•å¼‚å¸¸: {e}")
            
            # ğŸ”¥ v2.2: Plateauæ£€æµ‹ - æ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡
            plateau_scheduler.step(val_loss)

            # ä¿å­˜æœ€ä½³æ¨¡å‹ (v2.2: ä¿å­˜EMAæƒé‡)
            if val_psnr > best_psnr:
                best_psnr = val_psnr

                # åº”ç”¨EMAæƒé‡å†ä¿å­˜
                if ema is not None:
                    ema.store(model)
                    ema.apply_shadow(model)

                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'best_psnr': best_psnr,
                    'loss': val_loss,
                    'psnr': val_psnr,
                    'ssim': val_ssim,
                    'config': vars(args),
                    'train_history': train_history,
                    'ema_state': ema.ema if ema is not None else None  # v2.2: ä¿å­˜EMAçŠ¶æ€
                }, f"{args.save_dir}/models/best_model.pth")

                if ema is not None:
                    ema.restore(model)

                print(f"   ğŸ‰ æ–°çš„æœ€ä½³PSNR: {best_psnr:.2f}dB (EMAæ¨¡å‹å·²ä¿å­˜)")
        
        # å®šæœŸä¿å­˜
        if (epoch + 1) % args.save_freq == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_psnr': best_psnr,
                'config': vars(args),
                'train_history': train_history
            }, f"{args.save_dir}/models/epoch_{epoch+1}.pth")
        
        scheduler.step()
        print("-" * 60)
    
    # è®­ç»ƒå®Œæˆ
    total_time = time.time() - start_time
    print(f"\nğŸ‰ MambaIRv2-GPPNNè®­ç»ƒå®Œæˆ!")
    print(f"æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
    print(f"æœ€ä½³éªŒè¯PSNR: {best_psnr:.2f}dB")
    print(f"æ¨¡å‹ä¿å­˜è‡³: {args.save_dir}")
    
    # å®‰å…¨å…³é—­TensorBoard
    try:
        writer.close()
    except Exception as e:
        print(f"âš ï¸  TensorBoardå…³é—­å¼‚å¸¸: {e}")
    
    # ä¿å­˜é…ç½®å’Œç»“æœ (å‚è€ƒtrain_IRDN_migrated.py)
    results = {
        'config': vars(args),
        'best_psnr': best_psnr,
        'training_time': total_time,
        'total_params': total_params,
        'train_history': train_history,
        'save_dir': args.save_dir
    }
    
    import json
    with open(f"{args.save_dir}/results.json", 'w') as f:
        json.dump(results, f, indent=4, default=str)
    
    print(f"\nğŸ“‹ MambaIRv2-GPPNNè®­ç»ƒæŠ¥å‘Š:")
    print(f"   ç½‘ç»œ: MambaIRv2-GPPNN {args.model_size.upper()} ({total_params:,}å‚æ•°)")
    print(f"   æ•°æ®: Photoç›®å½• 600è®­ç»ƒ+50éªŒè¯+150æµ‹è¯• (å›ºå®šåˆ†å‰²)")
    print(f"   æŸå¤±: L1 + æ¢¯åº¦æŸå¤±")
    print(f"   æœ€ä½³PSNR: {best_psnr:.2f}dB")
    print(f"   è®­ç»ƒç”¨æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
    
    return results


if __name__ == '__main__':
    main()
