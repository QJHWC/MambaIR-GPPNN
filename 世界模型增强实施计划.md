# 世界模型增强实施计划

**File name**: 世界模型增强实施计划.md  
**Created**: 2025-10-23  
**Creator**: AI Assistant  
**Associated Protocol**: RIPER-5 + Multidimensional + Agent Protocol (Conditional Interactive Step Review Enhanced)

---

## 📋 任务描述 (Task Description)

**目标**: 将《最新任务计划.md》中定义的**FiWA-Diff世界模型增强方案**集成到现有MambaIR-GPPNN架构，实现从"像素映射器"到"世界一致生成器"的跨越。

**核心需求**:
1. 实现五大世界模型模块：WSM、DCA-FIM、DSC、WAC-X、Patch Prior
2. 保持现有架构和41项优化不变（向后兼容）
3. 所有模块可独立开关，便于对比实验
4. 预期性能提升：PSNR +0.5~1.0dB，SAM↓0.1~0.3°

**参考文档**:
- `最新任务计划.md`: 世界模型数学原理和伪代码
- `README.md`: 现有架构说明
- `OPTIMIZATION_V2.2_SUMMARY.md`: v2.2优化历史

---

## 🏗️ 项目概览 (Project Overview)

### 当前项目状态

**MambaIR-GPPNN v2.2**
- **架构**: MambaIRv2 (SSM) + GPPNN (渐进式融合) + 41项优化
- **性能**: Base-256 → PSNR 30.2dB, SSIM 0.85
- **显存**: Base-512 → 8GB, Large-512 → 16GB
- **训练时间**: Base-256 → 6-8h (80 epochs)

**核心模块**:
```
models/
├── mambair_gppnn.py          # 主网络（268行）
├── dual_modal_assm.py         # 双模态ASSM（308行）
├── cross_modal_attention.py   # 跨模态注意力（252行）
└── __init__.py

train.py                        # 训练脚本（866行）
config.py                       # 配置文件（222行）
```

---

## 🔍 分析阶段 (RESEARCH Mode - 已完成)

### 关键发现

#### 1. 代码架构分析
- **模块化程度高**: 各模块职责清晰，便于扩展
- **已有频域基础**: `train.py:94-100` 实现了FFT损失，可扩展为WAC-X
- **已有门控机制**: `dual_modal_assm.py:95-98` 可复用于WSM的gamma/beta
- **分块注意力**: `cross_modal_attention.py:78-95` 避免OOM，支持512×512

#### 2. 扩展点识别

| 扩展点 | 文件 | 行号 | 注入方式 |
|--------|------|------|---------|
| **WSM注入** | `mambair_gppnn.py` | 192 | Stage 1后调制特征 |
| **DCA增强** | `cross_modal_attention.py` | 112 | forward返回前对齐 |
| **DSC损失** | `train.py` | 613 | IRDN_Loss添加项 |
| **WAC-X损失** | `train.py` | 613 | IRDN_Loss添加项 |

#### 3. 依赖检查

**已有依赖（满足需求）**:
- ✅ `torch.nn.GRU` → WSM
- ✅ `torch.fft.rfft2` → WAC-X
- ✅ `F.grid_sample` → DCA-FIM

**无需额外安装**！

#### 4. 风险评估

| 风险项 | 影响 | 缓解措施 |
|--------|------|---------|
| 显存增加33% | Base-512: 8GB→10.6GB | 保持分块注意力策略 |
| 训练时间+28% | 6h→7.7h | 可接受，性能提升值得 |
| 新损失项冲突 | 梯度不稳定 | 使用任务计划推荐权重 |
| GRU隐状态管理 | 批次间传递复杂 | detach()避免长期累积 |

---

## 💡 方案评估 (INNOVATE Mode - 已完成)

### 对比三种方案

#### 方案一：渐进式集成 ⭐⭐⭐⭐⭐ **[已选定]**

**优势**:
- ✅ 向后兼容，不破坏现有优化
- ✅ 每个模块可独立开关验证
- ✅ 风险可控，失败不影响整体
- ✅ 实施周期短（6.5天）

**劣势**:
- ⚠️ 非完整的扩散模型范式
- ⚠️ 显存仍需增加33%

**预期效果**:
```
PSNR: 30.2dB → 31.0dB (+0.8dB)
SSIM: 0.85 → 0.88 (+0.03)
SAM: 2.5° → 2.2° (-0.3°)
显存: 8GB → 10.6GB (+33%)
训练时间: 6h → 7.7h (+28%)
```

#### 方案二：一体化重构（推翻重建）❌

**优势**:
- 完全对齐世界模型理论
- 扩散模型天然生成式先验

**劣势**:
- ❌ 41项优化全部作废
- ❌ 训练成本暴增（100-200 epochs）
- ❌ 显存翻倍（16GB+）
- ❌ 推理速度降低（DDPM多步采样）

**结论**: 🚫 不推荐，成本收益比极低

#### 方案三：混合式（部分重构）⚠️

**优势**:
- 保留MambaIR主干
- 获得部分扩散模型优势

**劣势**:
- ⚠️ 推理仍慢（30步采样）
- ⚠️ 显存+50%（12GB+）
- ⚠️ 训练复杂度高

**结论**: ⚠️ 可选，适合后续研究，不适合当前迭代

---

## 📐 详细实施计划 (PLAN Mode - 当前阶段)

### 实施策略

**总体原则**:
1. 分阶段实施，每阶段可独立验证
2. 所有新模块通过配置开关控制
3. 保持现有代码风格和命名规范
4. 每个模块都编写单元测试

**优先级排序**:
```
P1（核心）: WSM + DSC → 时序一致性 + 物理约束 → 2天
P2（增强）: DCA-FIM + WAC-X → 几何对齐 + 频域约束 → 2天  
P3（润色）: Patch Prior → 推理增强 → 0.5天
P4（集成）: 训练脚本 + 文档 → 2天
```

---

### Phase 1: 配置基础设施 (2小时)

#### 目标
建立世界模型配置管理框架

#### 文件清单
```
config.py                        [修改]
models/world_model/__init__.py   [新建]
```

#### 详细步骤

**步骤 1.1**: 扩展`config.py`

**位置**: `MambaIRv2_GPPNN_Config`类内部，添加新的配置字典

**代码**:
```python
# 在MambaIRv2_GPPNN_Config类中添加
WORLD_MODEL_CONFIG = {
    # ========== 模块开关 ==========
    'enable_world_model': False,     # 总开关
    'use_wsm': False,                # 世界状态记忆
    'use_dca_fim': False,            # 可形变对齐
    'use_dsc': False,                # 物理一致性损失
    'use_wacx': False,               # 频域一致性损失
    'use_patch_prior': False,        # Patch Prior
    
    # ========== 损失权重（参考任务计划默认值）==========
    'lambda_s': 0.3,                 # DSC权重
    'lambda_g': 0.05,                # DCA几何权重
    'lambda_w': 0.5,                 # WAC-X频域权重
    'lambda_p': 0.2,                 # Patch先验权重
    
    # ========== WSM参数 ==========
    'wsm_hidden_dim': 128,           # GRU隐状态维度
    'wsm_dropout': 0.1,              # Dropout率
    'wsm_layer_scale_init': 0.1,    # LayerScale初始值
    
    # ========== DCA-FIM参数 ==========
    'dca_num_points': 4,             # 形变采样点数量
    'dca_offset_groups': 1,          # 形变分组数
    'dca_deform_weight': 0.3,        # 形变特征融合权重
    
    # ========== DSC参数 ==========
    'dsc_mtf_kernel_size': 5,        # MTF卷积核大小
    'dsc_mtf_sigma': 1.0,            # 高斯模糊sigma
    'dsc_spectral_response': [0.299, 0.587, 0.114],  # RGB→PAN系数
    'dsc_lrms_weight': 0.3,          # LRMS损失权重
    
    # ========== WAC-X参数 ==========
    'wacx_interband_weight': 1.0,    # 跨带一致性权重
    'wacx_pan_gate_weight': 0.5,     # PAN门控权重
    'wacx_freq_threshold': 0.1,      # 高频阈值
    
    # ========== Patch Prior参数 ==========
    'patch_size': 32,                # Patch尺寸
    'patch_overlap': 0.25,           # Patch重叠率
    'patch_refiner_path': None,      # 预训练生成器路径
}

@classmethod
def get_world_model_config(cls):
    """获取世界模型配置"""
    return cls.WORLD_MODEL_CONFIG.copy()

@classmethod
def print_world_model_config(cls):
    """打印世界模型配置"""
    config = cls.get_world_model_config()
    if not config['enable_world_model']:
        print("🌍 世界模型增强: 未启用")
        return
    
    print("🌍 世界模型增强配置:")
    print(f"   WSM: {config['use_wsm']}")
    print(f"   DCA-FIM: {config['use_dca_fim']}")
    print(f"   DSC: {config['use_dsc']} (λs={config['lambda_s']})")
    print(f"   WAC-X: {config['use_wacx']} (λw={config['lambda_w']})")
    print(f"   Patch Prior: {config['use_patch_prior']} (λp={config['lambda_p']})")
```

**步骤 1.2**: 创建世界模型包结构

**文件**: `models/world_model/__init__.py`

**代码**:
```python
# -*- coding: utf-8 -*-
"""
世界模型增强模块
基于《最新任务计划.md》的五大核心组件

模块说明:
- WSM (World State Memory): 时序一致性，降低生成方差
- DCA-FIM (Deformable Cross-Attention): 几何对齐，减少配准误差
- DSC (Differentiable Sensor Consistency): 物理一致性，收紧光谱误差
- WAC-X (Wavelength-Agnostic Cross-band): 频域一致性，高频能量守恒
- Patch Prior Refiner: 生成流形约束，抑制伪影
"""

__version__ = '1.0.0'
__author__ = 'MambaIR-GPPNN Team'

# 延迟导入，避免循环依赖
def __getattr__(name):
    if name == 'WorldStateMemory':
        from .wsm import WorldStateMemory
        return WorldStateMemory
    elif name == 'DeformableCrossAttention':
        from .dca_fim import DeformableCrossAttention
        return DeformableCrossAttention
    elif name == 'SensorConsistencyLoss':
        from .sensor_loss import SensorConsistencyLoss
        return SensorConsistencyLoss
    elif name == 'WACXLoss':
        from .wacx_loss import WACXLoss
        return WACXLoss
    elif name == 'PatchPriorRefiner':
        from .patch_refiner import PatchPriorRefiner
        return PatchPriorRefiner
    raise AttributeError(f"module {__name__!r} has no attribute {name!r}")

__all__ = [
    'WorldStateMemory',
    'DeformableCrossAttention',
    'SensorConsistencyLoss',
    'WACXLoss',
    'PatchPriorRefiner',
]
```

---

### Phase 2: WSM模块 (1天)

#### 数学原理
```
h_t = GRU(Pool(F_t), h_{t-1})
gamma, beta = Linear(h_t)
F'_t = F_t * (1 + gamma * scale) + beta

效果: Var(x̃_0) = Var(x̂_0)(1 - ρ²) → MSE↓, PSNR↑
```

#### 文件清单
```
models/world_model/wsm.py        [新建, 150行]
models/mambair_gppnn.py          [修改, +15行]
tests/test_wsm.py                [新建, 测试]
```

#### 详细实现

**文件**: `models/world_model/wsm.py`

**完整代码**: (见PLAN阶段Phase 2详细内容，约150行)

**集成点1**: `models/mambair_gppnn.py` 的 `__init__` 方法

**位置**: 在`self._init_weights()`之前添加

**代码**:
```python
# 🔥 世界模型增强: WSM模块
self.use_wsm = kwargs.get('use_wsm', False)
if self.use_wsm:
    from .world_model import WorldStateMemory
    self.wsm = WorldStateMemory(
        feature_dim=embed_dim,
        hidden_dim=kwargs.get('wsm_hidden_dim', 128),
        dropout=kwargs.get('wsm_dropout', 0.1),
        layer_scale_init=kwargs.get('wsm_layer_scale_init', 0.1)
    )
    print(f"[Init] 🌍 世界状态记忆(WSM)已启用，hidden_dim={kwargs.get('wsm_hidden_dim', 128)}")
else:
    self.wsm = None
```

**集成点2**: `models/mambair_gppnn.py` 的 `forward` 方法

**位置**: 约第192行，Stage 1处理后

**代码**:
```python
# Stage 1: 全分辨率处理
ms_enhanced1, pan_enhanced1 = self.mamba_layers[0](ms_feat1, pan_feat1, (H, W))
ms_seq1 = ms_enhanced1.flatten(2).transpose(1, 2)
pan_seq1 = pan_enhanced1.flatten(2).transpose(1, 2)
fused_seq1 = self.cross_modal_layers[0](ms_seq1, pan_seq1)
fused_feat1 = fused_seq1.transpose(1, 2).reshape(B, self.embed_dim, H, W)

# 🔥 世界模型增强: WSM状态调制
if self.use_wsm and self.wsm is not None:
    h_prev = getattr(self, '_wsm_hidden_state', None)
    fused_feat1, h_t, gamma, beta = self.wsm(fused_feat1, h_prev)
    if self.training:
        self._wsm_hidden_state = h_t.detach()
    else:
        self._wsm_hidden_state = h_t

# 🔥 优化31: 边缘保护 + 低层细化
refined_feat1 = self.low_level_refine(fused_feat1)
# ... 后续代码不变
```

**测试脚本**: `tests/test_wsm.py`

```python
import torch
from models.world_model import WorldStateMemory

def test_wsm_forward():
    """测试WSM前向传播"""
    B, C, H, W = 2, 96, 64, 64
    feat = torch.randn(B, C, H, W)
    
    wsm = WorldStateMemory(feature_dim=C, hidden_dim=128)
    
    # 第一次调用（无隐状态）
    out1, h1, gamma1, beta1 = wsm(feat, h_prev=None)
    assert out1.shape == feat.shape
    assert h1.shape == (B, 128)
    assert gamma1.shape == (B, C)
    
    # 第二次调用（有隐状态）
    out2, h2, gamma2, beta2 = wsm(feat, h_prev=h1)
    assert out2.shape == feat.shape
    assert not torch.equal(h1, h2)  # 隐状态应该更新
    
    print("✅ WSM测试通过!")

if __name__ == "__main__":
    test_wsm_forward()
```

---

### Phase 3: DSC损失 (1天)

#### 数学原理
```
PAN_syn = MTF(Σ R_b * HRMS_b)
LRMS_syn = MTF(Downsample(HRMS))
L_DSC = ||PAN_syn - PAN_gt||₁ + 0.3||LRMS_syn - LRMS_gt||₁

效果: SAM↓, ERGAS↓
```

#### 文件清单
```
models/world_model/sensor_loss.py   [新建, 120行]
train.py                            [修改, +30行]
tests/test_dsc.py                   [新建, 测试]
```

#### 详细实现

**文件**: `models/world_model/sensor_loss.py`

**完整代码**: (见PLAN阶段Phase 3详细内容，约120行)

**集成点1**: `train.py` 的 `IRDN_Loss.__init__` 方法

**位置**: 在`__init__`末尾添加

**代码**:
```python
# 🔥 世界模型增强: DSC损失
self.use_dsc = kwargs.get('use_dsc', False)
if self.use_dsc:
    from models.world_model import SensorConsistencyLoss
    self.dsc_loss_fn = SensorConsistencyLoss(
        spectral_response=kwargs.get('dsc_spectral_response', [0.299, 0.587, 0.114]),
        mtf_kernel_size=kwargs.get('dsc_mtf_kernel_size', 5),
        mtf_sigma=kwargs.get('dsc_mtf_sigma', 1.0),
        lrms_weight=kwargs.get('dsc_lrms_weight', 0.3)
    )
    self.lambda_s = kwargs.get('lambda_s', 0.3)
else:
    self.dsc_loss_fn = None
```

**集成点2**: `train.py` 的 `IRDN_Loss.forward` 方法

**位置**: 修改签名并添加DSC计算

**修改前**:
```python
def forward(self, outputs, target):
```

**修改后**:
```python
def forward(self, outputs, target, **kwargs):
    """
    Args:
        outputs: [SR_1_4, SR_1_2, output_full]
        target: [B, C, H, W] GT
        **kwargs: 可选的pan_gt, ms_gt用于DSC/WAC-X
    """
```

**添加代码** (在return前):
```python
# 🔥 世界模型增强: DSC物理一致性损失
dsc_loss_val = torch.tensor(0.0, device=total_loss.device)
if self.use_dsc and self.dsc_loss_fn is not None:
    if 'pan_gt' in kwargs and 'ms_gt' in kwargs:
        dsc_losses = self.dsc_loss_fn(
            hrms_pred=output_full,
            pan_gt=kwargs['pan_gt'],
            lrms_gt=kwargs.get('ms_gt', None)
        )
        dsc_loss_val = dsc_losses['dsc_total']

# 更新总损失
total_loss = (self.alpha * total_l1 +
             self.beta * grad_loss +
             self.gamma * ssim_loss_val +
             edge_loss_val +
             freq_loss_val +
             self.lambda_s * dsc_loss_val)

return {
    'total_loss': total_loss,
    'l1_loss': total_l1,
    'grad_loss': grad_loss,
    'ssim_loss': ssim_loss_val,
    'edge_loss': edge_loss_val,
    'freq_loss': freq_loss_val,
    'dsc_loss': dsc_loss_val,  # 🔥 新增
    # ... 其他项
}
```

**集成点3**: `train.py` 的 `train_one_epoch` 函数

**位置**: 约第216行，损失计算部分

**修改前**:
```python
loss_dict = criterion(outputs, gt)
```

**修改后**:
```python
loss_dict = criterion(outputs, gt, pan_gt=pan, ms_gt=ms)
```

---

### Phase 4: DCA-FIM模块 (1.5天)

#### 数学原理
```
offset = ConvOffset(Q_lrms)
weight = Softmax(ConvWeight(Q_lrms))
V_aligned = DeformSample(V_pan, offset, weight)

效果: 配准误差↓ → PSNR↑
```

#### 文件清单
```
models/world_model/dca_fim.py       [新建, 150行]
models/cross_modal_attention.py    [修改, +20行]
tests/test_dca.py                   [新建, 测试]
```

#### 详细实现

**文件**: `models/world_model/dca_fim.py`

**完整代码**: (见PLAN阶段Phase 4详细内容，约150行)

**集成点**: `models/cross_modal_attention.py`

**位置1**: `__init__` 末尾添加

```python
# 🔥 世界模型增强: DCA-FIM
self.use_dca_fim = kwargs.get('use_dca_fim', False)
if self.use_dca_fim:
    from .world_model import DeformableCrossAttention
    self.dca_fim = DeformableCrossAttention(
        dim=dim,
        num_points=kwargs.get('dca_num_points', 4),
        offset_groups=kwargs.get('dca_offset_groups', 1),
        deform_weight=kwargs.get('dca_deform_weight', 0.3)
    )
else:
    self.dca_fim = None
```

**位置2**: `forward` 方法返回前

```python
import math

# 输出投影
fused_feat = self.proj(fused_feat)
fused_feat = self.proj_drop(fused_feat)

# 🔥 世界模型增强: DCA-FIM几何对齐
if self.use_dca_fim and self.dca_fim is not None:
    B, N, C = fused_feat.shape
    H = W = int(math.sqrt(N))
    ms_feat_2d = ms_feat.transpose(1, 2).reshape(B, C, H, W)
    pan_feat_2d = pan_feat.transpose(1, 2).reshape(B, C, H, W)
    fused_feat_2d = fused_feat.transpose(1, 2).reshape(B, C, H, W)
    
    aligned_feat_2d = self.dca_fim(fused_feat_2d, pan_feat_2d)
    fused_feat = aligned_feat_2d.flatten(2).transpose(1, 2)

return fused_feat
```

---

### Phase 5: WAC-X损失 (0.5天)

#### 数学原理
```
H_b = |FFT(HRMS_b)|
L_inter = Σ ||H_bi - H_bj||₁
G = norm(|HF(PAN)|)
L_gate = ||G ⊙ HF(HRMS)||₁

效果: 高频能量守恒 → 纹理真实↑
```

#### 文件清单
```
models/world_model/wacx_loss.py   [新建, 100行]
train.py                          [修改, +15行]
tests/test_wacx.py                [新建, 测试]
```

#### 详细实现

**文件**: `models/world_model/wacx_loss.py`

**完整代码**: (见PLAN阶段Phase 5详细内容，约100行)

**集成点**: `train.py` 的 `IRDN_Loss`

**位置1**: `__init__` 末尾

```python
# 🔥 世界模型增强: WAC-X损失
self.use_wacx = kwargs.get('use_wacx', False)
if self.use_wacx:
    from models.world_model import WACXLoss
    self.wacx_loss_fn = WACXLoss(
        interband_weight=kwargs.get('wacx_interband_weight', 1.0),
        pan_gate_weight=kwargs.get('wacx_pan_gate_weight', 0.5),
        freq_threshold=kwargs.get('wacx_freq_threshold', 0.1)
    )
    self.lambda_w = kwargs.get('lambda_w', 0.5)
else:
    self.wacx_loss_fn = None
```

**位置2**: `forward` 方法（在DSC后）

```python
# 🔥 世界模型增强: WAC-X频域一致性
wacx_loss_val = torch.tensor(0.0, device=total_loss.device)
if self.use_wacx and self.wacx_loss_fn is not None:
    if 'pan_gt' in kwargs:
        wacx_losses = self.wacx_loss_fn(hrms=output_full, pan=kwargs['pan_gt'])
        wacx_loss_val = wacx_losses['wacx_total']

# 更新总损失
total_loss = (self.alpha * total_l1 +
             self.beta * grad_loss +
             self.gamma * ssim_loss_val +
             edge_loss_val +
             freq_loss_val +
             self.lambda_s * dsc_loss_val +
             self.lambda_w * wacx_loss_val)

return {
    # ... 现有项
    'wacx_loss': wacx_loss_val,
}
```

---

### Phase 6: Patch Prior Refiner (0.5天)

#### 数学原理
```
L_patch = Σ_p min_z ||HRMS_p - G(z)||²

效果: 抑制伪影 → Q8↑, 主观质量↑
```

#### 文件清单
```
models/world_model/patch_refiner.py   [新建, 180行]
inference_with_world_model.py         [新建, 推理脚本]
tests/test_patch_refiner.py           [新建, 测试]
```

#### 详细实现

**文件**: `models/world_model/patch_refiner.py`

**完整代码**: (见PLAN阶段Phase 6详细内容，约180行)

**推理脚本**: `inference_with_world_model.py`

```python
# -*- coding: utf-8 -*-
"""
世界模型增强推理脚本
"""

import torch
import argparse
from models import create_mambairv2_gppnn
from models.world_model import PatchPriorRefiner
from data import create_photo_dataloaders

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_path', type=str, required=True)
    parser.add_argument('--use_patch_prior', action='store_true')
    parser.add_argument('--patch_size', type=int, default=32)
    args = parser.parse_args()
    
    # 加载模型
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = create_mambairv2_gppnn('base')
    checkpoint = torch.load(args.model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device).eval()
    
    # Patch Refiner
    refiner = None
    if args.use_patch_prior:
        refiner = PatchPriorRefiner(patch_size=args.patch_size)
        print("🌍 Patch Prior增强已启用")
    
    # 推理
    _, _, test_loader = create_photo_dataloaders(batch_size=1)
    
    with torch.no_grad():
        for idx, (ms, pan, gt) in enumerate(test_loader):
            ms, pan = ms.to(device), pan.to(device)
            
            # 标准推理
            _, _, output = model(ms, pan)
            
            # Patch Prior修正
            if refiner is not None:
                output = refiner.refine(output)
            
            # 保存结果
            # save_image(output, f"result_{idx}.png")
            
            if idx >= 10:
                break
    
    print("✅ 推理完成!")

if __name__ == "__main__":
    main()
```

---

### Phase 7: 训练脚本集成 (0.5天)

#### 文件清单
```
train.py            [修改, +50行]
train_unified.py    [修改, +30行]
```

#### 详细实现

**文件**: `train.py` 的 `main()` 函数

**添加参数**:
```python
# 世界模型参数
parser.add_argument('--enable_world_model', action='store_true',
                   help='🌍 启用世界模型增强（总开关）')
parser.add_argument('--use_wsm', action='store_true',
                   help='启用世界状态记忆')
parser.add_argument('--use_dca_fim', action='store_true',
                   help='启用可形变对齐')
parser.add_argument('--use_dsc', action='store_true',
                   help='启用物理一致性损失')
parser.add_argument('--use_wacx', action='store_true',
                   help='启用频域一致性损失')

# 损失权重
parser.add_argument('--lambda_s', type=float, default=0.3)
parser.add_argument('--lambda_w', type=float, default=0.5)
```

**修改模型创建**:
```python
# 创建模型
world_model_kwargs = {}
if args.enable_world_model:
    world_model_kwargs.update({
        'use_wsm': args.use_wsm,
        'use_dca_fim': args.use_dca_fim,
        'wsm_hidden_dim': 128,
        'dca_num_points': 4,
    })

model = create_mambairv2_gppnn(args.model_size, **world_model_kwargs).to(device)
```

**修改损失创建**:
```python
criterion = IRDN_Loss(
    alpha=1.0, beta=0.3, gamma=0.2,
    use_dsc=args.use_dsc,
    use_wacx=args.use_wacx,
    lambda_s=args.lambda_s,
    lambda_w=args.lambda_w,
)
```

---

### Phase 8: 文档和验证 (1天)

#### 文件清单
```
WORLD_MODEL_GUIDE.md              [新建, 使用指南]
README.md                         [修改, 添加章节]
quick_test_world_model.py         [新建, 快速测试]
experiments/compare_baseline.sh   [新建, 对比实验]
```

#### 详细实现

**文件**: `WORLD_MODEL_GUIDE.md`

```markdown
# 世界模型增强模块使用指南

## 快速开始

### 1. 启用所有模块（推荐）
\`\`\`bash
python train.py --model_size base --img_size 256 \
  --enable_world_model \
  --use_wsm --use_dca_fim --use_dsc --use_wacx
\`\`\`

### 2. 仅启用核心模块
\`\`\`bash
python train.py --model_size base --img_size 256 \
  --enable_world_model \
  --use_wsm --use_dsc
\`\`\`

### 3. 自定义损失权重
\`\`\`bash
python train.py --model_size base --img_size 256 \
  --enable_world_model \
  --use_dsc --lambda_s 0.5 \
  --use_wacx --lambda_w 0.8
\`\`\`

## 模块说明

### WSM (World State Memory)
- **功能**: 通过GRU隐状态维持时序一致性
- **效果**: PSNR +0.2dB, 方差↓
- **参数**: `--use_wsm`

### DCA-FIM (Deformable Cross-Attention)
- **功能**: 学习形变偏移实现亚像素级几何对齐
- **效果**: PSNR +0.3dB, 边缘伪影↓
- **参数**: `--use_dca_fim`

### DSC (Sensor Consistency)
- **功能**: 物理传感器一致性约束
- **效果**: SAM ↓0.2°, ERGAS↓
- **参数**: `--use_dsc --lambda_s 0.3`

### WAC-X (Cross-band Consistency)
- **功能**: 跨波段频域一致性约束
- **效果**: 纹理真实感↑, 高频保真
- **参数**: `--use_wacx --lambda_w 0.5`

### Patch Prior Refiner
- **功能**: 推理时Patch级流形修正
- **效果**: Q8↑, 主观质量↑
- **使用**: 见 `inference_with_world_model.py`

## 对比实验

### Baseline vs 世界模型
\`\`\`bash
# Baseline
python train.py --model_size base --img_size 256

# 世界模型完整版
python train.py --model_size base --img_size 256 \
  --enable_world_model --use_wsm --use_dca_fim --use_dsc --use_wacx
\`\`\`

### 消融实验
\`\`\`bash
# 仅WSM
python train.py --model_size base --img_size 256 --enable_world_model --use_wsm

# 仅DSC
python train.py --model_size base --img_size 256 --enable_world_model --use_dsc

# WSM+DSC
python train.py --model_size base --img_size 256 --enable_world_model --use_wsm --use_dsc
\`\`\`

## 预期效果

| 配置 | PSNR | SSIM | SAM | 显存 |
|------|------|------|-----|------|
| Baseline | 30.2dB | 0.85 | 2.5° | 8GB |
| +WSM | 30.4dB | 0.86 | 2.5° | 8.8GB |
| +WSM+DSC | 30.6dB | 0.87 | 2.3° | 9.2GB |
| Full | 31.0dB | 0.88 | 2.2° | 10.6GB |
```

**快速测试脚本**: `quick_test_world_model.py`

```python
"""快速测试所有世界模型模块是否可用"""

import torch
from models.world_model import (
    WorldStateMemory,
    DeformableCrossAttention,
    SensorConsistencyLoss,
    WACXLoss,
    PatchPriorRefiner
)

def test_all_modules():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    B, C, H, W = 2, 96, 64, 64
    
    # 测试WSM
    print("测试WSM...")
    feat = torch.randn(B, C, H, W).to(device)
    wsm = WorldStateMemory(C, 128).to(device)
    out, h, g, b = wsm(feat)
    assert out.shape == feat.shape
    print("✅ WSM通过")
    
    # 测试DCA-FIM
    print("测试DCA-FIM...")
    dca = DeformableCrossAttention(C, 4).to(device)
    aligned = dca(feat, feat)
    assert aligned.shape == feat.shape
    print("✅ DCA-FIM通过")
    
    # 测试DSC
    print("测试DSC...")
    hrms = torch.randn(B, 3, H, W).to(device)
    pan = torch.randn(B, 1, H, W).to(device)
    dsc = SensorConsistencyLoss().to(device)
    loss_dict = dsc(hrms, pan)
    assert 'dsc_total' in loss_dict
    print("✅ DSC通过")
    
    # 测试WAC-X
    print("测试WAC-X...")
    wacx = WACXLoss().to(device)
    loss_dict = wacx(hrms, pan)
    assert 'wacx_total' in loss_dict
    print("✅ WAC-X通过")
    
    # 测试Patch Prior
    print("测试Patch Prior...")
    refiner = PatchPriorRefiner(patch_size=32)
    refined = refiner.refine(hrms)
    assert refined.shape == hrms.shape
    print("✅ Patch Prior通过")
    
    print("\n🎉 所有模块测试通过!")

if __name__ == "__main__":
    test_all_modules()
```

---

## 📋 完整实施清单

### 按优先级排序的任务清单

#### Phase 1: 配置基础（预计2小时）
1. ☐ 修改`config.py`，添加WORLD_MODEL_CONFIG配置字典 **[review:true]**
2. ☐ 在`config.py`添加`get_world_model_config()`和`print_world_model_config()`方法 **[review:true]**
3. ☐ 创建`models/world_model/`目录 **[review:false]**
4. ☐ 创建`models/world_model/__init__.py`，设置延迟导入 **[review:true]**

#### Phase 2: WSM模块（预计1天）
5. ☐ 创建`models/world_model/wsm.py`，实现WorldStateMemory类（150行） **[review:true]**
6. ☐ 修改`models/mambair_gppnn.py`的`__init__`，添加WSM初始化 **[review:true]**
7. ☐ 修改`models/mambair_gppnn.py`的`forward`，在Stage 1后注入WSM调制 **[review:true]**
8. ☐ 创建`tests/test_wsm.py`，测试WSM前向传播和隐状态传递 **[review:true]**

#### Phase 3: DSC损失（预计1天）
9. ☐ 创建`models/world_model/sensor_loss.py`，实现SensorConsistencyLoss类（120行） **[review:true]**
10. ☐ 修改`train.py`的`IRDN_Loss.__init__`，添加DSC损失初始化 **[review:true]**
11. ☐ 修改`train.py`的`IRDN_Loss.forward`签名，支持`**kwargs` **[review:true]**
12. ☐ 修改`train.py`的`IRDN_Loss.forward`，计算并累加DSC损失 **[review:true]**
13. ☐ 修改`train.py`的`train_one_epoch`，传递`pan_gt`和`ms_gt` **[review:true]**
14. ☐ 创建`tests/test_dsc.py`，测试MTF卷积和光谱响应 **[review:true]**

#### Phase 4: DCA-FIM模块（预计1.5天）
15. ☐ 创建`models/world_model/dca_fim.py`，实现DeformableCrossAttention类（150行） **[review:true]**
16. ☐ 修改`models/cross_modal_attention.py`的`__init__`，添加DCA-FIM初始化 **[review:true]**
17. ☐ 修改`models/cross_modal_attention.py`的`forward`，添加`import math` **[review:false]**
18. ☐ 修改`models/cross_modal_attention.py`的`forward`，应用DCA-FIM对齐 **[review:true]**
19. ☐ 创建`tests/test_dca.py`，测试形变采样和grid_sample **[review:true]**

#### Phase 5: WAC-X损失（预计0.5天）
20. ☐ 创建`models/world_model/wacx_loss.py`，实现WACXLoss类（100行） **[review:true]**
21. ☐ 修改`train.py`的`IRDN_Loss.__init__`，添加WAC-X损失初始化 **[review:true]**
22. ☐ 修改`train.py`的`IRDN_Loss.forward`，计算并累加WAC-X损失 **[review:true]**
23. ☐ 创建`tests/test_wacx.py`，测试FFT频谱和门控逻辑 **[review:true]**

#### Phase 6: Patch Prior（预计0.5天）
24. ☐ 创建`models/world_model/patch_refiner.py`，实现PatchPriorRefiner类（180行） **[review:true]**
25. ☐ 创建`inference_with_world_model.py`推理脚本 **[review:true]**
26. ☐ 创建`tests/test_patch_refiner.py`，测试patch提取和合并 **[review:true]**

#### Phase 7: 训练脚本集成（预计0.5天）
27. ☐ 修改`train.py`的`main()`，添加世界模型命令行参数 **[review:true]**
28. ☐ 修改`train.py`的模型创建部分，传递世界模型配置kwargs **[review:true]**
29. ☐ 修改`train.py`的损失创建部分，传递损失权重kwargs **[review:true]**
30. ☐ 修改`train_unified.py`的`create_unified_args`，添加世界模型参数 **[review:true]**
31. ☐ 修改`train_unified.py`的`auto_configure`，添加预设逻辑 **[review:true]**

#### Phase 8: 文档和验证（预计1天）
32. ☐ 创建`WORLD_MODEL_GUIDE.md`使用指南文档 **[review:true]**
33. ☐ 更新`README.md`，添加世界模型增强章节 **[review:true]**
34. ☐ 创建`quick_test_world_model.py`快速测试脚本 **[review:true]**
35. ☐ 创建`experiments/compare_baseline.sh`对比实验脚本 **[review:true]**
36. ☐ 运行完整训练测试（Base-256, 10 epochs），验证功能正常 **[review:true]**
37. ☐ 运行对比实验：Baseline vs WSM vs WSM+DSC vs Full，记录指标 **[review:true]**

---

## 📊 预期效果和验收标准

### 性能指标

| 配置 | PSNR | SSIM | SAM | ERGAS | 显存(Base-512) | 训练时间 |
|------|------|------|-----|-------|---------------|---------|
| **Baseline** | 30.2dB | 0.85 | 2.5° | 3.2 | 8GB | 6h |
| **+WSM** | 30.4dB | 0.86 | 2.5° | 3.2 | 8.8GB | 6.3h |
| **+WSM+DSC** | 30.6dB | 0.87 | 2.3° | 3.1 | 9.2GB | 6.8h |
| **+WSM+DSC+DCA** | 30.8dB | 0.87 | 2.3° | 3.05 | 9.8GB | 7.2h |
| **Full** | **31.0dB** | **0.88** | **2.2°** | **3.0** | **10.6GB** | **7.7h** |

### 验收标准

#### ✅ 必须达到
1. 所有模块可独立开关，不影响baseline功能
2. 代码风格与现有项目一致
3. 所有单元测试通过
4. Full配置下PSNR提升≥0.5dB

#### ✅ 应该达到
1. Full配置下PSNR提升≥0.8dB
2. SAM降低≥0.2°
3. 显存增加≤40%
4. 训练时间增加≤35%

#### ✅ 可选达到
1. PSNR提升≥1.0dB
2. 主观质量明显提升（通过人工评估）
3. 推理速度不降低（不使用Patch Prior时）

---

## 📝 风险管理和应对策略

### 已识别风险

| 风险 | 概率 | 影响 | 缓解措施 | 应急预案 |
|------|------|------|---------|---------|
| 显存超限OOM | 中 | 高 | 保持分块注意力，测试显存占用 | 降低batch_size或禁用部分模块 |
| 新损失冲突 | 中 | 中 | 使用推荐权重，监控梯度范数 | 调整权重或禁用冲突损失 |
| GRU隐状态泄漏 | 低 | 中 | detach()断开梯度，定期重置 | 每个epoch重置隐状态 |
| DCA形变不收敛 | 低 | 中 | 预热期固定offset为0 | 禁用DCA或降低deform_weight |
| FFT数值不稳定 | 低 | 低 | 添加epsilon避免除零 | 使用log频谱或钳制范围 |

### 回滚策略

**如果Full配置无法达到预期效果**:
1. 回退到WSM+DSC配置（核心功能）
2. 禁用DCA-FIM和WAC-X（增强功能）
3. 仅使用Patch Prior作为推理增强（免训练）

**回滚命令**:
```bash
# 回滚到核心功能
python train.py --model_size base --img_size 256 \
  --enable_world_model --use_wsm --use_dsc

# 完全回滚到baseline
python train.py --model_size base --img_size 256
```

---

## 📅 时间线和里程碑

### Week 1 (Day 1-5)
- **Day 1**: Phase 1 配置基础 + Phase 2 WSM模块开发
- **Day 2**: Phase 2 WSM集成测试 + Phase 3 DSC损失开发
- **Day 3**: Phase 3 DSC集成测试 + Phase 4 DCA-FIM开发（前半）
- **Day 4**: Phase 4 DCA-FIM开发（后半）+ 集成测试
- **Day 5**: Phase 5 WAC-X损失 + Phase 6 Patch Prior

**里程碑1**: ✅ 核心模块（WSM+DSC）完成并测试通过

### Week 2 (Day 6-7)
- **Day 6**: Phase 7 训练脚本集成 + Phase 8 文档编写
- **Day 7**: Phase 8 完整训练测试 + 对比实验

**里程碑2**: ✅ 全部模块集成完成，性能达标

---

## 🎯 最终交付物清单

### 代码文件（新增/修改）
```
✅ models/world_model/__init__.py           [新建]
✅ models/world_model/wsm.py                [新建, 150行]
✅ models/world_model/sensor_loss.py        [新建, 120行]
✅ models/world_model/dca_fim.py            [新建, 150行]
✅ models/world_model/wacx_loss.py          [新建, 100行]
✅ models/world_model/patch_refiner.py      [新建, 180行]

✅ models/mambair_gppnn.py                  [修改, +20行]
✅ models/cross_modal_attention.py          [修改, +25行]
✅ train.py                                 [修改, +80行]
✅ train_unified.py                         [修改, +30行]
✅ config.py                                [修改, +60行]

✅ inference_with_world_model.py            [新建, 推理脚本]
✅ quick_test_world_model.py                [新建, 快速测试]
```

### 测试文件
```
✅ tests/test_wsm.py
✅ tests/test_dsc.py
✅ tests/test_dca.py
✅ tests/test_wacx.py
✅ tests/test_patch_refiner.py
```

### 文档文件
```
✅ WORLD_MODEL_GUIDE.md                     [新建, 使用指南]
✅ README.md                                [修改, 添加世界模型章节]
✅ 世界模型增强实施计划.md                    [本文档]
```

### 实验记录
```
✅ experiments/baseline_results.json        [Baseline指标]
✅ experiments/wsm_results.json             [WSM结果]
✅ experiments/full_results.json            [Full配置结果]
✅ experiments/compare_baseline.sh          [对比脚本]
```

---

## 📌 注意事项和最佳实践

### 开发规范
1. **代码风格**: 遵循现有项目的PEP8规范和命名习惯
2. **注释要求**: 关键算法添加数学原理注释
3. **类型提示**: 新函数添加类型注解（Python 3.8+）
4. **错误处理**: 添加try-except保护关键操作

### 测试规范
1. **单元测试**: 每个模块都有独立测试文件
2. **集成测试**: `quick_test_world_model.py`测试所有模块协同
3. **回归测试**: 确保baseline功能不受影响
4. **性能测试**: 记录显存和训练时间

### Git提交规范
```bash
# 功能开发
git commit -m "feat(world_model): 添加WSM模块 - 世界状态记忆"

# Bug修复
git commit -m "fix(world_model): 修复DCA-FIM的grid_sample边界处理"

# 文档更新
git commit -m "docs(world_model): 添加WORLD_MODEL_GUIDE使用指南"

# 测试添加
git commit -m "test(world_model): 添加WSM单元测试"
```

---

## 🔗 相关资源

### 参考文档
- 《最新任务计划.md》: 世界模型理论基础
- 《README.md》: 项目整体架构
- 《OPTIMIZATION_V2.2_SUMMARY.md》: 历史优化记录

### 学术参考
- MambaIR论文: https://github.com/csguoh/MambaIR
- GPPNN论文: Gradual Pansharpening Network
- Deformable Convolution: https://arxiv.org/abs/1703.06211

### 代码参考
- PyTorch Official: `F.grid_sample` 文档
- PyTorch Official: `torch.fft.rfft2` 文档
- PyTorch Official: `nn.GRUCell` 文档

---

## ✅ 签署和确认

**计划制定人**: AI Assistant  
**制定日期**: 2025-10-23  
**计划版本**: v1.0  
**预计完成日期**: 2025-10-30  

**关键决策**:
- ✅ 采用方案一：渐进式集成
- ✅ 优先实现WSM+DSC核心功能
- ✅ 所有模块可独立开关
- ✅ 保持向后兼容性

**风险确认**:
- ⚠️ 显存增加33%（可接受）
- ⚠️ 训练时间增加28%（可接受）
- ✅ 无破坏性变更风险

---

**计划状态**: ✅ 已完成，等待执行批准

**下一步**: 进入EXECUTE模式，开始逐项实施代码修改

---

*本计划遵循RIPER-5 + Multidimensional Thinking + Agent Execution Protocol*

