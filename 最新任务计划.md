# 🌍 FiWA-Diff 世界模型增强版：优化思路与实现示例

## 1. 优化总体思路

本方案旨在将世界模型（World Model）的思想引入 FiWA-Diff 遥感超分辨率架构，使模型从「像素映射器」进化为「理解场景逻辑的生成器」。  
核心目标是在保持原有高指标（PSNR、SSIM、SAM、ERGAS、Q8）的基础上，引入世界一致性约束与可解释机制，使生成结果更稳定、更真实、更符合物理规律。

### 🧠 五大模块概述

| 模块 | 类型 | 物理/数学意义 | 对指标影响 |
|------|------|---------------|-------------|
| **WSM** 世界状态记忆 | 时序一致性 | 方差缩减（Var↓） | PSNR↑、SSIM↑ |
| **DCA-FIM** 可形变对齐 | 几何一致性 | 配准误差项减少 | PSNR↑、边缘伪影↓ |
| **DSC** 可微传感链 | 光谱一致性 | SAM上界收紧 | SAM↓、ERGAS↓ |
| **WAC-X** 跨带频域一致 | 频谱一致性 | 高频能量守恒 | PSNR↑、纹理真实 |
| **GeoPos + Patch Prior** 世界先验 | 泛化与结构流形 | 泛化误差上界↓ | Q8↑、主观质量↑ |

### 🌐 统一优化目标（MAP形式）

\[
\min_{\hat I}\|\hat I-I^\star\|_2^2
+\lambda_s\mathcal{R}_{\text{sens}}
+\lambda_g\mathcal{R}_{\text{geom}}
+\lambda_w\mathcal{R}_{\text{wac-x}}
+\lambda_p\mathcal{R}_{\text{patch}},
\quad
\hat I\in\mathcal{C}(h_t,\text{GeoPos})
\]

其中：
- \( \mathcal{R}_{\text{sens}} \)：可微物理一致性（DSC）  
- \( \mathcal{R}_{\text{geom}} \)：几何一致性（DCA）  
- \( \mathcal{R}_{\text{wac-x}} \)：频域一致性（WAC-X）  
- \( \mathcal{R}_{\text{patch}} \)：生成流形约束（Patch Prior）  
- \( \mathcal{C}(h_t, \text{GeoPos}) \)：由世界状态 \(h_t\) 和几何编码控制的可行集。

---

## 2. 数学推导摘要

### 2.1 DSC: 可微传感链一致性 → 光谱误差抑制

\[
\mathcal{R}_{\text{sens}}=
\|\mathsf{MTF}(\sum_b r_b\hat I_b)-PAN\|_1
+\alpha\|\mathsf{MTF}(D\!\downarrow(\hat I))-LRMS\|_1
\]

\[
r^\top(\hat I-I^\star)\to 0
\Rightarrow \text{SAM}(\hat I,I^\star)\downarrow,\;\text{ERGAS}\downarrow
\]

---

### 2.2 DCA-FIM: 形变补偿对齐 → PSNR/SSIM 提升

\[
I^\star(x+\mathbf{u}) \approx I^\star(x)+\nabla I^\star(x)\cdot\mathbf{u},
\quad
\epsilon'=\nabla I^\star\cdot(\mathbf{u}-\hat{\mathbf{u}})
\Rightarrow \mathrm{MSE}\downarrow
\]

---

### 2.3 WSM: 世界状态记忆 → 方差缩减稳定生成

\[
h_t=\Phi(h_{t+1},\mathrm{Pool}(F_t)),
\quad
\mathrm{Var}(\tilde x_0)=\mathrm{Var}(\hat x_0)(1-\rho^2)
\Rightarrow \mathrm{MSE}\downarrow,\;\text{PSNR}\uparrow
\]

---

### 2.4 WAC-X: 跨带频域一致 + PAN门控 → 高频真实

\[
\mathcal{R}_{\text{wac-x}}=
\sum_{b_1\neq b_2}\|H_{b_1}-H_{b_2}\|_1
+\beta\,\|G\odot \mathrm{HF}(\hat I)\|_1
\]

其中 \(G=\mathrm{norm}(|\mathrm{HF}(PAN)|)\)。  
→ 高频误差能量下降 ⇒ PSNR↑、伪影↓。

---

### 2.5 Patch Prior: 流形约束 → 抑制过锐化伪影

\[
\mathcal{R}_{\text{patch}}=\sum_p\min_z\|\hat I_p-G(z)\|^2,
\quad
\mathbb{E}\|\hat I-I^\star\|^2=\text{bias}^2+\text{variance}\downarrow
\]

---

## 3. 模块实现示例代码

### 3.1 World State Memory (WSM)

```python
class WorldStateMemory(nn.Module):
    def __init__(self, d):
        super().__init__()
        self.cell = nn.GRUCell(d, d)
        self.to_gamma = nn.Linear(d, d)
        self.to_beta = nn.Linear(d, d)

    def forward(self, feat, h_next):
        pooled = feat.mean(dim=[2,3])
        h_t = self.cell(pooled, h_next)
        gamma, beta = self.to_gamma(h_t), self.to_beta(h_t)
        return h_t, gamma, beta
```

**调用方式：**

```python
h = torch.zeros(B, C, device=x.device)
for t in timesteps:
    feat = encoder(x_t, cond)
    h, g, b = wsm(feat, h)
    x_t = decoder(feat, gamma=g, beta=b)
```

---

### 3.2 Deformable Cross-Attention (DCA-FIM)

```python
class DCAModule(nn.Module):
    def __init__(self, dim, kpoints=4):
        super().__init__()
        self.offset = nn.Conv2d(dim, 2*kpoints, 3, 1, 1)
        self.weight = nn.Conv2d(dim, kpoints, 3, 1, 1)
        self.conv_out = nn.Conv2d(dim, dim, 1)

    def forward(self, q_lrms, k_pan, v_pan):
        off = self.offset(q_lrms)
        w = torch.softmax(self.weight(q_lrms), dim=1)
        sampled = deformable_sample(v_pan, off)  # 双线性采样
        out = (w.unsqueeze(-1)*sampled).sum(1)
        return self.conv_out(out)
```

---

### 3.3 Differentiable Sensor Consistency (DSC)

```python
def sensor_forward(hrms, R, mtf_kernel):
    pan_syn = (hrms * R.view(1,-1,1,1)).sum(1, keepdim=True)
    return F.conv2d(pan_syn, mtf_kernel, padding="same")

def dsc_loss(hrms, PAN, LRMS, R, mtf):
    L_pan = F.l1_loss(sensor_forward(hrms, R, mtf), PAN)
    L_lrms = F.l1_loss(sensor_forward(F.avg_pool2d(hrms, 4), R, mtf), LRMS)
    return L_pan + 0.3 * L_lrms
```

---

### 3.4 Patch Prior Refiner (免训练)

```python
class PatchPriorRefiner:
    def __init__(self, generator, patch_size=32):
        self.gen = generator.eval().requires_grad_(False)
        self.k = patch_size

    def loss(self, x):
        patches = x.unfold(2,self.k,self.k).unfold(3,self.k,self.k)
        patches = patches.contiguous().view(-1,3,self.k,self.k)
        recon = self.gen(patches)
        return F.mse_loss(recon, patches)
```

---

## 4. 训练与推理流程

### 4.1 训练阶段

```python
loss_total = loss_diff + \
             λs*dsc_loss(hr, PAN, LR, R, mtf) + \
             λg*dca_loss(features_lr, features_pan) + \
             λw*wacx_loss(hr, lr, PAN) + \
             λp*refiner.loss(hr)
loss_total.backward()
optimizer.step()
```

---

### 4.2 推理阶段（免训练增强）

```python
with torch.no_grad():
    hr = model.infer(LRMS, PAN)
    hr = DSC_refine(hr, PAN, LRMS)     # 可微物理一致修正
    hr = WACX_gate(hr, PAN)            # 高频门控
    hr = PatchRefine(hr, generator)    # Patch 级修补
save_image(hr, "output_sr.png")
```

---

## 5. 实验预期与理论保证

| 指标              | 数学依据                       | 预期变化         |
| --------------- | -------------------------- | ------------ |
| **PSNR / SSIM** | (DCA)+(WSM)+(WAC-X) 减少 MSE | +0.2~+0.4 dB |
| **SAM**         | (DSC) 收紧谱角上界               | ↓0.1~0.3°    |
| **ERGAS**       | DSC + WAC-X 改善全局一致性        | ↓0.05~0.15   |
| **Q8 / 视觉质量**   | Patch Prior 流形约束           | ↑纹理自然性       |
| **泛化性**         | 冻结 GeoPos + 世界先验降低复杂度      | ↑稳定性         |

---

## 6. 模型创新总结

"我们首次将世界模型的隐状态记忆、物理一致性和生成式先验引入遥感超分辨率任务，实现了从像素映射到世界一致生成的跨越。"

### ✅ 特点

- 不破坏原 FiWA-Diff 接口结构
- 可逐模块启用
- 大部分增强可 **免训练**
- 提供明确的物理与数学解释
- 提升客观指标与主观质量

---

## 7. 推荐默认参数

| 参数       | 含义         | 默认值   |
| -------- | ---------- | ----- |
| λs       | DSC 权重     | 0.3   |
| λg       | DCA 几何权重   | 0.05  |
| λw       | WAC-X 频域权重 | 0.5   |
| λp       | Patch 先验权重 | 0.2   |
| 隐状态维度    | h_t 大小     | 128   |
| Patch 尺寸 | refiner 输入 | 32×32 |

---

## 📘 最佳实践建议

- 在训练前冻结 ViT/DINO 语义编码器
- 在推理阶段启用 DSC + WAC-X + PatchRefine
- 用 EMA + Cosine LR 保障稳定
- 输出日志监控 SAM / ERGAS 改善趋势
